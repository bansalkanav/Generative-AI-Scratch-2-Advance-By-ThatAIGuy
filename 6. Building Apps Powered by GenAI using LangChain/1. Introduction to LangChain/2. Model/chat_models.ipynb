{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a23aea24-ce81-4540-8b2e-bf4e09fd6a67",
   "metadata": {},
   "source": [
    "# **Models - LLM(Legacy) and ChatModel**\n",
    "\n",
    "- A model can be a **LLM(Legacy)** or a **ChatModel**.\n",
    "- LLMs(Legacy) handle various language operations such as translation, summarization, question answering, and content creation. \n",
    "- Modern LLMs are typically accessed through a chat model interface that takes a list of messages as input and returns a message as output. Chat Models are customized for conversational usage. **[Click Here](https://python.langchain.com/docs/integrations/chat/)** to check the complete list of LLMs which can be used with LangChain.\n",
    "- The output of a ChatModel (and therefore, of this chain) is a message.\n",
    "\n",
    "The newest generation of chat models offer additional capabilities:\n",
    "1. [Tool calling](https://python.langchain.com/docs/concepts/tool_calling/): Many popular chat models offer a native tool calling API. This API allows developers to build rich applications that enable LLMs to interact with external services, APIs, and databases. Tool calling can also be used to extract structured information from unstructured data and perform various other tasks.\n",
    "2. [Structured output](https://python.langchain.com/docs/concepts/structured_outputs/): A technique to make a chat model respond in a structured format, such as JSON that matches a given schema.\n",
    "3. [Multimodality](https://python.langchain.com/docs/concepts/multimodality/): The ability to work with data other than text; for example, images, audio, and video.\n",
    "\n",
    "### **Integrations**\n",
    "LangChain has many chat model integrations that allow you to use a wide variety of models from different providers. These integrations are one of two types:\n",
    "\n",
    "1. **Official models:** These are models that are officially supported by LangChain and/or model provider. You can find these models in the **`langchain-<provider>`** packages.\n",
    "2. **Community models:** There are models that are mostly contributed and supported by the community. You can find these models in the **`langchain-community`** package.\n",
    "\n",
    "LangChain chat models are named with a convention that prefixes \"Chat\" to their class names (e.g., ChatOllama, ChatAnthropic, ChatOpenAI, etc.).\n",
    "\n",
    "\n",
    "### **Key Methods**\n",
    "The key methods of a chat model are:\n",
    "\n",
    "1. invoke: The primary method for interacting with a chat model. It takes a list of messages as input and returns a list of messages as output.\n",
    "2. stream: A method that allows you to stream the output of a chat model as it is generated.\n",
    "3. batch: A method that allows you to batch multiple requests to a chat model together for more efficient processing.\n",
    "4. bind_tools: A method that allows you to bind a tool to a chat model for use in the model's execution context.\n",
    "5. with_structured_output: A wrapper around the invoke method for models that natively support structured output.\n",
    "\n",
    "### **Standard Parameters**\n",
    "\n",
    "Standard parameters are currently only enforced on integrations that have their own integration packages (e.g. langchain-openai, langchain-anthropic, etc.), they're not enforced on models in langchain-community.\n",
    "\n",
    "| Parameter      | Description |\n",
    "|--------------|-------------|\n",
    "| model        | The name or identifier of the specific AI model you want to use (e.g., \"gpt-3.5-turbo\" or \"gpt-4\"). |\n",
    "| temperature  | Controls the randomness of the model's output. A higher value (e.g., 1.0) makes responses more creative, while a lower value (e.g., 0.0) makes them more deterministic and focused. |\n",
    "| timeout      | The maximum time (in seconds) to wait for a response from the model before canceling the request. Ensures the request doesnâ€™t hang indefinitely. |\n",
    "| max_tokens   | Limits the total number of tokens (words and punctuation) in the response. This controls how long the output can be. |\n",
    "| stop         | Specifies stop sequences that indicate when the model should stop generating tokens. For example, you might use specific strings to signal the end of a response. |\n",
    "| max_retries  | The maximum number of attempts the system will make to resend a request if it fails due to issues like network timeouts or rate limits. |\n",
    "| api_key      | The API key required for authenticating with the model provider. This is usually issued when you sign up for access to the model. |\n",
    "| base_url     | The URL of the API endpoint where requests are sent. This is typically provided by the model's provider and is necessary for directing your requests. |\n",
    "| rate_limiter | An optional BaseRateLimiter to space out requests to avoid exceeding rate limits. See rate-limiting below for more details. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806bfea3-54f1-40f4-a08e-5e951df2ddf8",
   "metadata": {},
   "source": [
    "## **OpenAI Chat Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb318b67-fd03-4aab-ac8a-fe15c0231d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install langchain-openai -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4402b36a-ced8-4157-a393-32f1a56f0913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup API Key\n",
    "\n",
    "f = open('keys/.openai_api_key.txt')\n",
    "\n",
    "OPENAI_API_KEY = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0745ecff-bf27-4e51-8562-1b68c3cc3d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kanavbansal/Developer/.env_jupyter/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Why did the data scientist break up with the statistician? \\n\\nBecause she found him mean and not very significant!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 15, 'total_tokens': 39, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'stop', 'logprobs': None}, id='run-2a2e01eb-97a8-4d93-ae2f-71e1e192a80e-0', usage_metadata={'input_tokens': 15, 'output_tokens': 24, 'total_tokens': 39, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import OpenAI ChatModel\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Set the OpenAI Key and initialize a ChatModel\n",
    "chat_model = ChatOpenAI(api_key=OPENAI_API_KEY, model=\"gpt-4o-mini\", temperature=1)\n",
    "\n",
    "prompt = \"Tell me a short joke about Data Science\"\n",
    "\n",
    "chat_model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc81099d-8426-4f8e-8e9d-0cccd4915da3",
   "metadata": {},
   "source": [
    "## **Google GenAI - Chat Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1698394c-12ae-4f5a-bf2f-f731f10a61dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install langchain-google-genai -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e380313c-365e-4a48-b96b-9828f6ae9f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup API Key\n",
    "\n",
    "f = open('keys/.gemini.txt')\n",
    "\n",
    "GOOGLE_API_KEY = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "079087d1-4adc-437a-bd8e-899e8a255294",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1740421668.630050 1980547 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Why did the data scientist break up with the statistician?\\n\\nBecause they kept arguing about whether correlation implied causation!', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-f32d6ce0-44bc-4ef4-9e88-2a7214939393-0', usage_metadata={'input_tokens': 8, 'output_tokens': 24, 'total_tokens': 32, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Google ChatModel\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Set the OpenAI Key and initialize a ChatModel\n",
    "chat_model = ChatGoogleGenerativeAI(api_key=GOOGLE_API_KEY, model=\"gemini-2.0-flash-exp\", temperature=1)\n",
    "\n",
    "prompt = \"Tell me a short joke about Data Science\"\n",
    "\n",
    "chat_model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2dee74-1dac-47de-9f6d-6472435e4b5a",
   "metadata": {},
   "source": [
    "## **HuggingFace**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f53c92e-c423-478d-8a4c-befa1dd93c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain-huggingface -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c5c234e-8502-4b20-af67-434d0d01057f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kanavbansal/Developer/.env_jupyter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"What should I study to become a data scientist?\\n\\nAs for studying, I am sure that with enough time to study it. After the year 6, study 1 will help create some interesting data. It is very important for me to learn about things like the structure of data and to see some data as it changes in the course of the study.\\n\\nYou have a very interesting idea of how you want to go about learning data science. What will you do?\\n\\nIf I have good skills in the field I can come up with the way I want to learn in less time. It will not be a perfect and I can try very hard to understand what I want it to be like. But I hope that it doesn't make it impossible for me.\\n\\nIf I fail in the learning I can try to take my time but at the same time I will have a lot more time. Even then it isn't as good as working on data in school or study 1 with other people. Besides that, in my experience many people are unhappy with data. Even if I succeed I can still be successful, even if I fail I am still working on it.\\n\\nAt the end of our interview, she talked to me about her experience with data analysis. She explained that after getting into data science she still had lots of problems with it. She said that she tried to study this stuff and made good use of it.\\n\\nDo you have any plans to teach any course outside data science in the past year?\\n\\nI would like to continue to try and teach data and related subjects at my own end of this work. Before I do you might ask. For example, if there is a book/video and no one likes it just a few paragraphs of what you do. If that happens I have to tell them and I won't. The whole question will depend.\\n\\nHow long does it take for you to become a data scientist in Canada?\\n\\nIt depends on how much we spend on research. It depends. It depends how much we are willing to pay because what is new is not new. I feel that the longer it is the better. I am also confident I will succeed in going to a higher level of data science and possibly even become a data scientist.\\n\\nPlease describe how much does it cost you to obtain study 1 data for your study?\\n\\nWe offer it for free to the public. We don't take payment or credit for data collected in study 1. We offer it for free for a long time. We are asking\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_id = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "pipe = pipeline(\n",
    "    task=\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=512\n",
    ")\n",
    "\n",
    "hf = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "hf.invoke(\"What should I study to become a data scientist?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
