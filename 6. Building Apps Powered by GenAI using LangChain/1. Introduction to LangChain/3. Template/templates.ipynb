{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d97cd4b2-0127-48ea-8237-641c715f88c9",
   "metadata": {},
   "source": [
    "# **Templates**\n",
    "**(PromptTemplate and ChatPromptTemplate)**\n",
    "\n",
    "<img src=\"images/langchain_model_io.jpg\">\n",
    "\n",
    "\n",
    "<img src=\"images/langchain_LCEL.JPG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd999c2-5a6b-49bb-a0d8-902ec9e72623",
   "metadata": {},
   "source": [
    "## **PromptTemplate**\n",
    "\n",
    "Input to a model can be a **string** value or **chat message** list.\n",
    "\n",
    "**Prompt Template**  \n",
    "- Prompt Templates are used to convert raw user input to a better input to the LLM.\n",
    "- Templates allow us to easily configure and modify our input prompts to LLM calls.\n",
    "- A template may include instructions, few-shot examples, and specific context and questions appropriate for a given task.\n",
    "- LangChain provides tooling to create and work with prompt templates.\n",
    "- LangChain strives to create model agnostic templates to make it easy to reuse existing templates across different language models.\n",
    "- Typically, language models expect the prompt to either be a string or else a list of chat messages.\n",
    "\n",
    "\n",
    "Prompt templates are used to convert raw user input to a better input to the LLM or ChatModels.  \n",
    "Templates offer a more systematic approach to passing in variables to prompts for models, instead of using f-string literals or .format() calls. The PromptTemplate converts these into function parameter names that we can pass in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f63bb6-dc92-4a62-bd85-7a61989b9321",
   "metadata": {},
   "source": [
    "### **Creating a PromptTemplate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6e2ef49-861a-4249-be5c-d36d06cdb69e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kanavbansal/Developer/.env_jupyter/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['adjective', 'content'], input_types={}, partial_variables={}, template='Tell me a {adjective} joke about {content}.')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Creating a prompt template with input variables\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"Tell me a {adjective} joke about {content}.\"\n",
    ")\n",
    "\n",
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8bb94d5-c45c-4eed-9e74-43cd323cebff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adjective', 'content']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check the input variables\n",
    "\n",
    "prompt_template.input_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4357d6-0223-47ad-8d81-666d6f839958",
   "metadata": {},
   "source": [
    "### **PromptTemplate - .format(), .format_messages() and .format_prompt()**\n",
    "\n",
    "Templates offer a more systematic approach to passing in variables to prompts for models, instead of using f-string literals or .format() calls. The PromptTemplate converts these into function parameter names that we can pass in.\n",
    "\n",
    "- .format(): Converts the PromptTemplate to `String`\n",
    "- .format_message(): Converts the PromptTemplate to list of `ChatMessages`\n",
    "- .format_prompt(): Converts the PromptTempate to `StringPromptValue`. `PromptValues` can be converted to both LLM (to string) inputs and ChatModel (to messages) inputs. On this we can apply `to_messages()` or `to_string()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbcb0059-7e24-4e0c-bd13-71fc8b6e6697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "Tell me a funny joke about chickens.\n"
     ]
    }
   ],
   "source": [
    "# format() returns a string\n",
    "\n",
    "prompt = prompt_template.format(adjective=\"funny\", content=\"chickens\")\n",
    "\n",
    "print(type(prompt))\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c58a1e07-59f4-4ff2-ab0d-2a87023dc6bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.prompt_values.StringPromptValue'>\n",
      "text='Tell me a funny joke about chickens.'\n"
     ]
    }
   ],
   "source": [
    "# format_prompt() returns a string i.e. StingPromptValue\n",
    "# PromptValue can be converted to either Strings or Messages\n",
    "\n",
    "prompt = prompt_template.format_prompt(adjective=\"funny\", content=\"chickens\")\n",
    "\n",
    "print(type(prompt))\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6aeec0c1-146a-433e-85e4-cb5ce5aa12e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[HumanMessage(content='Tell me a funny joke about chickens.', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "## We can use StingPromptValue and convert it to List of ChatMessages\n",
    "\n",
    "prompt = prompt_template.format_prompt(adjective=\"funny\", content=\"chickens\").to_messages()\n",
    "\n",
    "print(type(prompt))\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fd2d02b-1c5e-4845-97ab-9bcf8acb3e5d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PromptTemplate' object has no attribute 'format_messages'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[43mprompt_template\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_messages\u001b[49m(adjective\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunny\u001b[39m\u001b[38;5;124m\"\u001b[39m, content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchickens\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(prompt))\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(prompt)\n",
      "File \u001b[0;32m~/Developer/.env_jupyter/lib/python3.9/site-packages/pydantic/main.py:828\u001b[0m, in \u001b[0;36mBaseModel.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[0;32m--> 828\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PromptTemplate' object has no attribute 'format_messages'"
     ]
    }
   ],
   "source": [
    "prompt = prompt_template.format_messages(adjective=\"funny\", content=\"chickens\")\n",
    "\n",
    "print(type(prompt))\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a3bfb9-4cba-4db4-9b60-ac12b08d2853",
   "metadata": {},
   "source": [
    "## **PromptTemplate, partial_variables and .invoke()**\n",
    "\n",
    "Think of `partial_variables` as default function parameters.\n",
    "\n",
    "`PromptTemplate` and `ChatPromptTemplate` implement the Runnable interface, the basic building block of the **LangChain Expression Language (LCEL)**. This means they support `invoke`, `ainvoke`, `stream`, `astream`, `batch`, `abatch`, `astream_log` calls.\n",
    "\n",
    "**Using `invoke()`:**  \n",
    "`PromptTemplate` **accepts a dictionary (of the prompt variables)** and returns a `StringPromptValue`.  \n",
    "A `ChatPromptTemplate` **accepts a dictionary** and returns a `ChatPromptValue`.\n",
    "\n",
    "`PromptValues` can be converted to both LLM (to string) inputs and ChatModel (to messages) inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34ac7d38-bae9-4207-acae-6ca07ba3170d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['content'], input_types={}, partial_variables={'adjective': 'funny'}, template='Tell me a {adjective} joke about {content}.')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Creating a prompt template with input variables\n",
    "prompt_template = PromptTemplate(\n",
    "    template=\"Tell me a {adjective} joke about {content}.\",\n",
    "    partial_variables={\"adjective\": \"funny\"}\n",
    ")\n",
    "\n",
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a578c162-686c-45e5-a991-90918bd563b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='Tell me a dark joke about data science.')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_input = {\"adjective\": \"dark\", \"content\": \"data science\"}\n",
    "\n",
    "prompt_template.invoke(raw_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d425f6f-88a9-461b-b34f-b70f631719f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='Tell me a funny joke about data science.')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_input = {\"content\": \"data science\"}\n",
    "\n",
    "prompt_template.invoke(raw_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ab60b4-68a8-4c0c-864d-4eff3e7c0c29",
   "metadata": {},
   "source": [
    "## **ChatPromptTemplate**\n",
    "\n",
    "The prompt to chat models is a list of chat messages.\n",
    "\n",
    "Each chat message is associated with content, and an additional parameter called role. For example, in the OpenAI Chat Completions API, a chat message can be associated with an AI assistant, a human or a system role."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bba8600-1dbe-4006-8c5e-5ab5ae077c28",
   "metadata": {},
   "source": [
    "### **ChatPromptTemplate and from_template()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4a736db-8c78-439a-8c94-5d6c2b7b4d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_template(\n",
    "    \"What is {topic}?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51f7c21c-93ba-4cfb-ba5b-338835325024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['topic']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_template.input_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85412fc6-a79e-4eb7-a98a-d776930a7837",
   "metadata": {},
   "source": [
    "### **ChatPromptTemplate - .format(), .format_messages() and .format_prompt()**\n",
    "\n",
    "Templates offer a more systematic approach to passing in variables to prompts for models, instead of using f-string literals or .format() calls. The PromptTemplate converts these into function parameter names that we can pass in.\n",
    "\n",
    "- .format(): Converts the PromptTemplate to `String`\n",
    "- .format_message(): Converts the PromptTemplate to list of `ChatMessages`\n",
    "- .format_prompt(): Converts the PromptTempate to `ChatPromptValue`. `PromptValues` can be converted to both LLM (to string) inputs and ChatModel (to messages) inputs. On this we can apply `to_messages()` or `to_string()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22a49ce6-dc0d-4ad5-88e7-01315c61d88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "Human: What is machine learning?\n"
     ]
    }
   ],
   "source": [
    "# format() returns a string\n",
    "\n",
    "prompt = chat_template.format(topic=\"machine learning\")\n",
    "\n",
    "print(type(prompt))\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e079d5a5-a54c-4dbe-9e26-329d0c81b515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[HumanMessage(content='What is machine learning?', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "# fomat_messages() return list of chat messages\n",
    "\n",
    "prompt = chat_template.format_messages(topic=\"machine learning\")\n",
    "\n",
    "print(type(prompt))\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98e119b0-2261-4d38-a01c-eff90fb2dbe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.prompt_values.ChatPromptValue'>\n",
      "messages=[HumanMessage(content='What is machine learning?', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "# format_prompt() returns a chat here i.e. ChatPromptValue()\n",
    "# PromptValue can be converted to either Strings or Chat Messages\n",
    "\n",
    "prompt = chat_template.format_prompt(topic=\"machine learning\")\n",
    "\n",
    "print(type(prompt))\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c39e2c-2b73-49ad-9c05-fb778f60796b",
   "metadata": {},
   "source": [
    "## **ChatPromptTemplate and from_messages()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "927ffc3f-c3c2-4ef2-9edf-1c7d77bb8d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
    "        (\"human\", \"Hello, how are you doing?\"),\n",
    "        (\"ai\", \"I'm doing well, thanks!\"),\n",
    "        (\"human\", \"{user_input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07784763-9403-4a92-83af-c9be3800e1ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name', 'user_input']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_template.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26b093e8-2626-49a1-9e5a-69abaaa2e52d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"System: You are a helpful AI bot. Your name is Bob.\\nHuman: Hello, how are you doing?\\nAI: I'm doing well, thanks!\\nHuman: What is your name?\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_template.format(name=\"Bob\", user_input=\"What is your name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad1ae895-54ad-4eaa-a80c-149bb54dbecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful AI bot. Your name is Bob.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Hello, how are you doing?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"I'm doing well, thanks!\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='What is your name?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# format_messages() returns chat messages \n",
    "\n",
    "chat_template.format_messages(name=\"Bob\", user_input=\"What is your name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02998b4b-db2a-4c66-a0b6-6b39c5c5132b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful AI bot. Your name is Bob.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Hello, how are you doing?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"I'm doing well, thanks!\", additional_kwargs={}, response_metadata={}), HumanMessage(content='What is your name?', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# format_prompt() returns a chat here i.e. ChatPromptValue()\n",
    "\n",
    "chat_template.format_prompt(name=\"Bob\", user_input=\"What is your name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44455b32-c2ec-4b95-b9b8-4742ae5e2f95",
   "metadata": {},
   "source": [
    "## **ChatPromptTemplate, partial_variables and .invoke()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb153c83-8650-4d0b-ba84-10eea476428e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['user_input'], input_types={}, partial_variables={'name': 'Alice'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['name'], input_types={}, partial_variables={}, template='You are a helpful AI bot. Your name is {name}.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='Hello, how are you doing?'), additional_kwargs={}), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template=\"I'm doing well, thanks!\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['user_input'], input_types={}, partial_variables={}, template='{user_input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate(\n",
    "    messages = [\n",
    "        (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
    "        (\"human\", \"Hello, how are you doing?\"),\n",
    "        (\"ai\", \"I'm doing well, thanks!\"),\n",
    "        (\"human\", \"{user_input}\"),\n",
    "    ], \n",
    "    partial_variables={\"name\": \"Alice\"}\n",
    ")\n",
    "\n",
    "chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c336781-66cf-47fa-94aa-3d3292a3215e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful AI bot. Your name is Alice.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Hello, how are you doing?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"I'm doing well, thanks!\", additional_kwargs={}, response_metadata={}), HumanMessage(content='What is your name?', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_input = {\"user_input\": \"What is your name?\"}\n",
    "\n",
    "chat_template.invoke(raw_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "914b508a-9f7a-474d-a674-3b7a1e04a6ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You are a helpful AI bot. Your name is Alice.\n",
      "Human: Hello, how are you doing?\n",
      "AI: I'm doing well, thanks!\n",
      "Human: What is your name?\n"
     ]
    }
   ],
   "source": [
    "print(chat_template.invoke(raw_input).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "581cf03f-8e4e-42a1-86d0-af4d946895b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='You are a helpful AI bot. Your name is Alice.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Hello, how are you doing?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"I'm doing well, thanks!\", additional_kwargs={}, response_metadata={}), HumanMessage(content='What is your name?', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "print(chat_template.invoke(raw_input).to_messages())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025f36f2-9f16-4239-9572-a9ca72eec8df",
   "metadata": {},
   "source": [
    "## **Designing ChatPromptTemplate using SystemMessagePromptTemplate, HumanMessagePromptTemplate and AIMessagePromptTemplate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "02925376-d6fb-4055-a254-4d1fb9b9384b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful AI bot. Your name is Alice.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Hello, how are you doing?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"I'm doing well, thanks!\", additional_kwargs={}, response_metadata={}), HumanMessage(content='What is your name?', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate, AIMessagePromptTemplate\n",
    "\n",
    "# System Template\n",
    "system_prompt_template = SystemMessagePromptTemplate.from_template(\"You are a helpful AI bot. Your name is {name}.\")\n",
    "\n",
    "# Human Template\n",
    "human_prompt_template = HumanMessagePromptTemplate.from_template(\"Hello, how are you doing?\")\n",
    "\n",
    "# AI Template\n",
    "ai_prompt_template = AIMessagePromptTemplate.from_template(\"I'm doing well, thanks!\")\n",
    "\n",
    "# Human Template\n",
    "human_prompt_template_input = HumanMessagePromptTemplate.from_template(\"{user_input}\")\n",
    "\n",
    "# Compile a chat prompt\n",
    "chat_template = ChatPromptTemplate(\n",
    "    messages=[ system_prompt_template, \n",
    "               human_prompt_template, \n",
    "               ai_prompt_template, \n",
    "               human_prompt_template_input ],\n",
    "    partial_variables={\"name\": \"Alice\"}\n",
    ")\n",
    "\n",
    "raw_input = {\"user_input\": \"What is your name?\"}\n",
    "\n",
    "chat_template.invoke(raw_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016d6a4a-ba99-40a2-9e94-9fc77392f67b",
   "metadata": {},
   "source": [
    "### **In above implementation, we can use HumanMessage and AIMessage as shown below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a176193c-e70d-4722-885f-acb41c633ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['name', 'user_input']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful AI bot. Your name is Bob.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Hello, how are you doing?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"I'm doing well, thanks!\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='What is your name?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "# System Template\n",
    "system_prompt_template = SystemMessagePromptTemplate.from_template(\"You are a helpful AI bot. Your name is {name}.\")\n",
    "\n",
    "# Human Template\n",
    "human_prompt = HumanMessage(\"Hello, how are you doing?\")\n",
    "\n",
    "# AI Template\n",
    "ai_prompt = AIMessage(\"I'm doing well, thanks!\")\n",
    "\n",
    "# Human Template\n",
    "human_prompt_template = HumanMessagePromptTemplate.from_template(\"{user_input}\")\n",
    "\n",
    "# Compile a chat prompt\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [system_prompt_template, human_prompt, ai_prompt, human_prompt_template]\n",
    ")\n",
    "\n",
    "print(chat_template.input_variables)\n",
    "\n",
    "chat_template.format_messages(name=\"Bob\", user_input=\"What is your name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a439d488-ec08-4d1d-9ce3-eb5189ab362b",
   "metadata": {},
   "source": [
    "## **MessagesPlaceholder**\n",
    "\n",
    "A placeholder which can be used to pass in a list of messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9718cfa7-6c90-4166-a5b8-1b15ee476248",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'chat_history'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MessagesPlaceholder\n\u001b[1;32m      3\u001b[0m prompt \u001b[38;5;241m=\u001b[39m MessagesPlaceholder(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat_history\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mprompt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# raises KeyError\u001b[39;00m\n",
      "File \u001b[0;32m~/Developer/.env_jupyter/lib/python3.9/site-packages/langchain_core/prompts/chat.py:240\u001b[0m, in \u001b[0;36mMessagesPlaceholder.format_messages\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformat_messages\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[BaseMessage]:\n\u001b[1;32m    226\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Format messages from kwargs.\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \n\u001b[1;32m    228\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;124;03m        ValueError: If variable is not a list of messages.\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    237\u001b[0m     value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    238\u001b[0m         kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariable_name, [])\n\u001b[1;32m    239\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptional\n\u001b[0;32m--> 240\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvariable_name\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    241\u001b[0m     )\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    243\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    244\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvariable \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariable_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m should be a list of base messages, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    245\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m         )\n",
      "\u001b[0;31mKeyError\u001b[0m: 'chat_history'"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "prompt = MessagesPlaceholder(\"chat_history\")\n",
    "prompt.format_messages() # raises KeyError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "554af0a0-6655-49bc-bfe8-67e9b9d03b61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = MessagesPlaceholder(\"chat_history\", optional=True)\n",
    "\n",
    "prompt.format_messages() # returns empty list []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "07e0b78e-0b16-4b01-9c23-760356777cd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are an AI assistant.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Hello!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.format_messages(\n",
    "    chat_history=[\n",
    "        (\"system\", \"You are an AI assistant.\"),\n",
    "        (\"human\", \"Hello!\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8757b1c0-1874-4a21-9d78-80a916708087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Hello!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Limiting the number of messages\n",
    "\n",
    "prompt = MessagesPlaceholder(\"chat_history\", optional=True, n_messages=1)\n",
    "\n",
    "prompt.format_messages(\n",
    "    chat_history=[\n",
    "        (\"system\", \"You are an AI assistant.\"),\n",
    "        (\"human\", \"Hello!\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f338db3d-1070-473b-8b8c-a0159d25c38c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['name', 'user_input'], optional_variables=['chat_history'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x115fa8e50>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={'chat_history': []}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['name'], input_types={}, partial_variables={}, template='You are a helpful AI bot. Your name is {name}.'), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['user_input'], input_types={}, partial_variables={}, template='{user_input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# Compile a chat prompt\n",
    "chat_template = ChatPromptTemplate(\n",
    "    messages=[ (\"system\", \"You are a helpful AI bot. Your name is {name}.\"), \n",
    "     MessagesPlaceholder(variable_name=\"chat_history\", optional=True), \n",
    "     (\"human\", \"{user_input}\") ]\n",
    ")\n",
    "\n",
    "chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fbc6364a-4b7c-4ffe-af5c-f706e41376e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful AI bot. Your name is Bob.', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is your name?', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_input = {\"name\": \"Bob\", \"user_input\": \"What is your name?\"}\n",
    "\n",
    "chat_template.invoke(raw_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3c595134-abe0-4d09-a6c3-9a421915619d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Human Message\n",
    "human_prompt = HumanMessage(content=\"Hello, how are you doing?\")\n",
    "\n",
    "# AI Message\n",
    "ai_prompt = AIMessage(content=\"I'm doing well, thanks!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8f0559cd-37e0-4e35-820d-27484090f76b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful AI bot. Your name is Bob.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Hello, how are you doing?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"I'm doing well, thanks!\", additional_kwargs={}, response_metadata={}), HumanMessage(content='What is your name?', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_input = {\"name\": \"Bob\", \"user_input\": \"What is your name?\", \"chat_history\": [human_prompt, ai_prompt]}\n",
    "\n",
    "chat_template.invoke(raw_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8bccd6e8-a2d5-435c-a80f-bdf96713a101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful AI bot. Your name is Bob.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Hello, how are you doing?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"I'm doing well, thanks!\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='What is your name?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_template.invoke(raw_input).to_messages()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
