{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6184a554-1851-4a31-abf1-87c2561f29be",
   "metadata": {},
   "source": [
    "# **LCEL and Chains**\n",
    "\n",
    "**LangChain Expression Language** allows to build compositions of chains by putting components together. It leverages the `|` pipe symbol to compose LangChain components.\n",
    "\n",
    "**Chains** refer to sequences of calls - whether to an LLM, a tool, or a data preprocessing step. The primary supported way to do this is with LCEL.\n",
    "\n",
    "**Chains** allows us to link the output of one LLM call as the input of another call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa15d575-4524-41b8-a4a1-f3da57950631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup API Key\n",
    "\n",
    "f = open('keys/.openai_api_key.txt')\n",
    "\n",
    "OPENAI_API_KEY = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c3e157e-dd82-4541-925e-70a640a3723c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kanavbansal/Developer/.env_jupyter/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Set the OpenAI Key and initialize a ChatModel\n",
    "chat_model = ChatOpenAI(openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "069c2668-961a-43ba-a5ee-ff85b0297cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"\"\"You are a helpful AI bot who expertise in Data Science Tutor. \n",
    "                      You are known to make any complex topic simpler even for a beginner.\"\"\"),\n",
    "        (\"human\", \"What is {topic}?\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afb84660-ae5b-485c-b2f7-b6dbd9788b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af9b77b3-15b6-4969-81db-ca93e76d7476",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = chat_template | chat_model | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb9fd74-86a7-4fce-82b8-fffca74189dc",
   "metadata": {},
   "source": [
    "### **chain.invoke()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1211818b-5272-4c83-8545-27189b9ca614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature selection is the process of selecting a subset of relevant features (variables, predictors) from the original set of features in a dataset. The goal of feature selection is to improve the performance of a machine learning model by reducing overfitting, increasing model interpretability, and reducing computation time.\n",
      "\n",
      "There are three main types of feature selection techniques:\n",
      "1. Filter methods: These methods select features based on their statistical properties, such as correlation with the target variable or variance. Common techniques include Pearson correlation coefficient and chi-square test.\n",
      "2. Wrapper methods: These methods evaluate different subsets of features using a specific machine learning algorithm and select the subset that produces the best performance. Examples include forward selection, backward elimination, and recursive feature elimination.\n",
      "3. Embedded methods: These methods perform feature selection as part of the model building process. Techniques such as Lasso regression and decision trees inherently perform feature selection by penalizing or pruning irrelevant features.\n",
      "\n",
      "Feature selection is crucial for building a more efficient and accurate machine learning model by focusing on the most important features and reducing noise in the data.\n"
     ]
    }
   ],
   "source": [
    "user_input = {\"topic\": \"Feature Selection\"}\n",
    "\n",
    "response = chain.invoke(user_input)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f928262-571d-44d2-8885-ba7d4baea364",
   "metadata": {},
   "source": [
    "### **chain.stream()**\n",
    "\n",
    "Large language models can take several seconds to generate a complete response to a query. This is far slower than the **~200-300 ms threshold** at which an application feels responsive to an end user.\n",
    "\n",
    "The key strategy to make the application feel more responsive is to show intermediate progress; viz., to stream the output from the model **token by token**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1aca568-22e8-47fa-9a13-ff9327eed232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature selection is the process of selecting a subset of relevant features (variables, predictors) from the original set of features in a dataset. The goal of feature selection is to improve model performance by reducing overfitting, decreasing computational complexity, and increasing model interpretability.\n",
      "\n",
      "There are various techniques for feature selection, including:\n",
      "\n",
      "1. Filter methods: These methods select features based on statistical measures like correlation, chi-squared test, or mutual information.\n",
      "\n",
      "2. Wrapper methods: These methods evaluate different subsets of features using a specific machine learning algorithm to determine the best subset.\n",
      "\n",
      "3. Embedded methods: These methods perform feature selection as part of the model building process, where feature importance is determined during training.\n",
      "\n",
      "By selecting the most relevant features, we can improve the efficiency and accuracy of machine learning models, leading to better predictions and insights from the data."
     ]
    }
   ],
   "source": [
    "for chunk in chain.stream(user_input):\n",
    "    print(chunk, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58d3611-b2c1-4cef-8a32-9cc9fb6c08d7",
   "metadata": {},
   "source": [
    "## **Case Study**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "117dbb42-bab0-4347-abab-c22f280f7436",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "279c1f99-9b85-4607-a71f-ce890c0b0630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Research Report: Understanding the Transformer Algorithm\n",
      "\n",
      "## Abstract\n",
      "The Transformer algorithm has revolutionized the field of natural language processing (NLP) and machine learning since its introduction in 2017. This report provides a comprehensive overview of the architecture, mechanisms, and applications of the Transformer model, highlighting its significance in various domains.\n",
      "\n",
      "## 1. Introduction\n",
      "The Transformer model, introduced by Vaswani et al. in the paper \"Attention is All You Need,\" has become a foundational architecture for many state-of-the-art NLP tasks. Unlike previous models that relied heavily on recurrent neural networks (RNNs) and convolutional neural networks (CNNs), the Transformer utilizes a self-attention mechanism that allows for parallelization and improved performance on long-range dependencies in data.\n",
      "\n",
      "## 2. Architecture of the Transformer\n",
      "The Transformer architecture consists of an encoder-decoder structure, each composed of multiple layers. \n",
      "\n",
      "### 2.1 Encoder\n",
      "The encoder's role is to process the input data and generate a set of continuous representations. Each encoder layer consists of two main components:\n",
      "- **Multi-Head Self-Attention Mechanism**: This allows the model to weigh the importance of different words in a sentence relative to each other. It computes attention scores using three vectors: Query (Q), Key (K), and Value (V).\n",
      "- **Feed-Forward Neural Network**: After the attention mechanism, the output is passed through a feed-forward network, which applies a non-linear transformation.\n",
      "\n",
      "### 2.2 Decoder\n",
      "The decoder generates the output sequence from the encoder's representations. It also consists of multiple layers, with the following components:\n",
      "- **Masked Multi-Head Self-Attention**: This prevents the decoder from attending to future tokens in the sequence, ensuring that predictions are made based only on past tokens.\n",
      "- **Encoder-Decoder Attention**: This layer allows the decoder to focus on relevant parts of the encoder's output.\n",
      "- **Feed-Forward Neural Network**: Similar to the encoder, the decoder also includes a feed-forward network.\n",
      "\n",
      "### 2.3 Positional Encoding\n",
      "Since the Transformer does not inherently understand the order of tokens, positional encodings are added to the input embeddings to provide information about the position of each token in the sequence.\n",
      "\n",
      "## 3. Self-Attention Mechanism\n",
      "The self-attention mechanism is the core innovation of the Transformer. It computes attention scores for each token in relation to all other tokens in the input sequence. The attention score is calculated as follows:\n",
      "\n",
      "1. **Calculate Q, K, V**: For each input token, compute the Query, Key, and Value vectors.\n",
      "2. **Compute Attention Scores**: The attention score for a pair of tokens is computed using the dot product of their Query and Key vectors, followed by a softmax operation to normalize the scores.\n",
      "3. **Weighted Sum**: The output for each token is a weighted sum of the Value vectors, where the weights are the attention scores.\n",
      "\n",
      "This mechanism allows the model to capture contextual relationships between words, regardless of their distance in the sequence.\n",
      "\n",
      "## 4. Training the Transformer\n",
      "Transformers are typically trained using large datasets and optimized using techniques such as:\n",
      "- **Adam Optimizer**: A variant of stochastic gradient descent that adapts the learning rate.\n",
      "- **Learning Rate Scheduling**: Gradually adjusting the learning rate during training to improve convergence.\n",
      "- **Regularization Techniques**: Such as dropout to prevent overfitting.\n",
      "\n",
      "## 5. Applications of Transformers\n",
      "Transformers have been successfully applied in various domains, including:\n",
      "- **Natural Language Processing**: Tasks such as translation, summarization, and sentiment analysis.\n",
      "- **Computer Vision**: Vision Transformers (ViTs) have been developed for image classification and object detection.\n",
      "- **Speech Processing**: Used in automatic speech recognition and text-to-speech systems.\n",
      "\n",
      "## 6. Conclusion\n",
      "The Transformer algorithm has fundamentally changed the landscape of machine learning and NLP. Its ability to handle long-range dependencies and parallelize computations has led to significant advancements in various applications. As research continues, the Transformer architecture is likely to evolve further, paving the way for even more sophisticated models.\n",
      "\n",
      "## References\n",
      "1. Vaswani, A., Shard, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., & Polosukhin, I. (2017). Attention is All You Need. *Advances in Neural Information Processing Systems*, 30.\n",
      "2. Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. *arXiv preprint arXiv:1810.04805*.\n",
      "3. Dosovitskiy, A., & Brox, T. (2020). Inverting VGG Image Encoders with a Deep Convolutional Network. *arXiv preprint arXiv:2004.02900*.\n",
      "4. Liu, Y., Ott, M., Goyal, N., Du, J., & Joshi, M. (2019). RoBERTa: A Robustly Optimized BERT Pretraining Approach. *arXiv preprint arXiv:1907.11692*.\n",
      "5. Dosovitskiy, A., & Brox, T. (2021). Vision Transformers. *arXiv preprint arXiv:2010.11929*.\n"
     ]
    }
   ],
   "source": [
    "WRITER_SYS_PROMPT = \"\"\"You are a research assistant and scientific writer.\n",
    "You take in requests about the topics and write organized research reports on those topics.\n",
    "Also you share the appropriate references at the end of report.\"\"\"\n",
    "\n",
    "HUMAN_PROMPT_1 = \"\"\"Write an organized research report about {topic}.\"\"\"\n",
    "\n",
    "writer_chat_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", WRITER_SYS_PROMPT), \n",
    "    (\"human\", HUMAN_PROMPT_1)\n",
    "])\n",
    "\n",
    "\n",
    "writer_chat_model = ChatOpenAI(openai_api_key=OPENAI_API_KEY,\n",
    "                               model=\"gpt-4o-mini\",\n",
    "                               temperature=0.0)\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "writer_chain = writer_chat_template | writer_chat_model | output_parser\n",
    "\n",
    "research_report = writer_chain.invoke({\"topic\": \"how transformers algorithm works?\"})\n",
    "\n",
    "print(research_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9dba82f2-cbca-41a9-a356-824641669866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- **Clarity and Structure**: The report is well-structured and clearly outlines the key components of the Transformer algorithm, making it accessible for readers with varying levels of expertise in NLP and machine learning.\n",
      "\n",
      "- **Depth of Content**: While the report provides a solid overview of the architecture and mechanisms, it could benefit from deeper exploration of the implications of the self-attention mechanism and its advantages over RNNs and CNNs.\n",
      "\n",
      "- **Examples and Applications**: The applications section is a strong point; however, including specific case studies or examples of successful implementations would enhance the practical understanding of the Transformer's impact across different domains.\n",
      "\n",
      "- **References and Citations**: The references are relevant and up-to-date, but it would be beneficial to include more recent studies or advancements in Transformer models post-2021 to reflect ongoing developments in the field.\n",
      "\n",
      "- **Future Directions**: The conclusion mentions the potential evolution of Transformer architectures, but discussing specific emerging trends or challenges in the field would provide a more comprehensive outlook for future research.\n"
     ]
    }
   ],
   "source": [
    "REVIEWER_SYS_PROMPT = \"\"\"You are a reviewer for research reports. \n",
    "You take in research reports and provide a feedback on them.\"\"\"\n",
    "\n",
    "HUMAN_PROMPT_2 = \"\"\"Provide feedback as 5 concise bullet points on this research report: \n",
    "\n",
    "{input}\"\"\"\n",
    "\n",
    "reviewer_chat_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", REVIEWER_SYS_PROMPT), \n",
    "    (\"human\", HUMAN_PROMPT_2)\n",
    "])\n",
    "\n",
    "reviewer_chat_model = ChatOpenAI(openai_api_key=OPENAI_API_KEY, \n",
    "                                 model=\"gpt-4o-mini\", \n",
    "                                 temperature=0.2)\n",
    "\n",
    "reviewer_chain = reviewer_chat_template | reviewer_chat_model | output_parser\n",
    "\n",
    "report_feedback = reviewer_chain.invoke({\"input\": research_report})\n",
    "\n",
    "print(report_feedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d89dd0a7-1c3c-46d7-ba2e-8f57b875da91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Research Report: Understanding the Transformer Algorithm\n",
      "\n",
      "## Abstract\n",
      "The Transformer algorithm has revolutionized the field of natural language processing (NLP) and machine learning since its introduction in 2017. This report provides a comprehensive overview of the architecture, mechanisms, and applications of the Transformer model, highlighting its significance in various domains. Additionally, it explores the implications of the self-attention mechanism, presents case studies of successful implementations, and discusses emerging trends and challenges in the field.\n",
      "\n",
      "## 1. Introduction\n",
      "The Transformer model, introduced by Vaswani et al. in the seminal paper \"Attention is All You Need,\" has become a foundational architecture for many state-of-the-art NLP tasks. Unlike previous models that relied heavily on recurrent neural networks (RNNs) and convolutional neural networks (CNNs), the Transformer utilizes a self-attention mechanism that allows for parallelization and improved performance on long-range dependencies in data. This report aims to elucidate the architecture and mechanisms of the Transformer, while also examining its transformative impact across various applications.\n",
      "\n",
      "## 2. Architecture of the Transformer\n",
      "The Transformer architecture consists of an encoder-decoder structure, each composed of multiple layers. \n",
      "\n",
      "### 2.1 Encoder\n",
      "The encoder's role is to process the input data and generate a set of continuous representations. Each encoder layer consists of two main components:\n",
      "- **Multi-Head Self-Attention Mechanism**: This allows the model to weigh the importance of different words in a sentence relative to each other. It computes attention scores using three vectors: Query (Q), Key (K), and Value (V). This mechanism enables the model to capture contextual relationships between words, regardless of their distance in the sequence, which is a significant advantage over RNNs that process tokens sequentially.\n",
      "- **Feed-Forward Neural Network**: After the attention mechanism, the output is passed through a feed-forward network, which applies a non-linear transformation.\n",
      "\n",
      "### 2.2 Decoder\n",
      "The decoder generates the output sequence from the encoder's representations. It also consists of multiple layers, with the following components:\n",
      "- **Masked Multi-Head Self-Attention**: This prevents the decoder from attending to future tokens in the sequence, ensuring that predictions are made based only on past tokens.\n",
      "- **Encoder-Decoder Attention**: This layer allows the decoder to focus on relevant parts of the encoder's output.\n",
      "- **Feed-Forward Neural Network**: Similar to the encoder, the decoder also includes a feed-forward network.\n",
      "\n",
      "### 2.3 Positional Encoding\n",
      "Since the Transformer does not inherently understand the order of tokens, positional encodings are added to the input embeddings to provide information about the position of each token in the sequence. This addition is crucial for maintaining the sequential nature of language.\n",
      "\n",
      "## 3. Self-Attention Mechanism\n",
      "The self-attention mechanism is the core innovation of the Transformer. It computes attention scores for each token in relation to all other tokens in the input sequence. The attention score is calculated as follows:\n",
      "\n",
      "1. **Calculate Q, K, V**: For each input token, compute the Query, Key, and Value vectors.\n",
      "2. **Compute Attention Scores**: The attention score for a pair of tokens is computed using the dot product of their Query and Key vectors, followed by a softmax operation to normalize the scores.\n",
      "3. **Weighted Sum**: The output for each token is a weighted sum of the Value vectors, where the weights are the attention scores.\n",
      "\n",
      "This mechanism allows the model to capture contextual relationships between words, regardless of their distance in the sequence, providing a significant advantage over RNNs and CNNs, which struggle with long-range dependencies.\n",
      "\n",
      "## 4. Training the Transformer\n",
      "Transformers are typically trained using large datasets and optimized using techniques such as:\n",
      "- **Adam Optimizer**: A variant of stochastic gradient descent that adapts the learning rate.\n",
      "- **Learning Rate Scheduling**: Gradually adjusting the learning rate during training to improve convergence.\n",
      "- **Regularization Techniques**: Such as dropout to prevent overfitting.\n",
      "\n",
      "## 5. Applications of Transformers\n",
      "Transformers have been successfully applied in various domains, including:\n",
      "- **Natural Language Processing**: Tasks such as translation, summarization, and sentiment analysis. For instance, Google's BERT model has significantly improved the accuracy of search queries by understanding context better.\n",
      "- **Computer Vision**: Vision Transformers (ViTs) have been developed for image classification and object detection, demonstrating competitive performance with traditional CNNs.\n",
      "- **Speech Processing**: Used in automatic speech recognition and text-to-speech systems, enhancing the quality and accuracy of voice interfaces.\n",
      "\n",
      "### Case Studies\n",
      "1. **BERT in Search Engines**: The implementation of BERT in Google Search has improved the understanding of user queries, leading to more relevant search results.\n",
      "2. **ViTs in Medical Imaging**: Vision Transformers have been applied in medical imaging for tasks such as tumor detection, showing promising results in accuracy and efficiency.\n",
      "\n",
      "## 6. Future Directions\n",
      "The Transformer architecture is likely to evolve further, with ongoing research focusing on:\n",
      "- **Efficiency Improvements**: Techniques such as sparse attention and model distillation aim to reduce the computational burden of Transformers.\n",
      "- **Multimodal Learning**: Integrating text, image, and audio data to create more robust models capable of understanding complex inputs.\n",
      "- **Ethical Considerations**: Addressing biases in training data and ensuring fairness in model predictions.\n",
      "\n",
      "## 7. Conclusion\n",
      "The Transformer algorithm has fundamentally changed the landscape of machine learning and NLP. Its ability to handle long-range dependencies and parallelize computations has led to significant advancements in various applications. As research continues, the Transformer architecture is poised for further innovations, addressing emerging challenges and expanding its impact across diverse fields.\n",
      "\n",
      "## References\n",
      "1. Vaswani, A., Shard, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., & Polosukhin, I. (2017). Attention is All You Need. *Advances in Neural Information Processing Systems*, 30.\n",
      "2. Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. *arXiv preprint arXiv:1810.04805*.\n",
      "3. Dosovitskiy, A., & Brox, T. (2020). Inverting VGG Image Encoders with a Deep Convolutional Network. *arXiv preprint arXiv:2004.02900*.\n",
      "4. Liu, Y., Ott, M., Goyal, N., Du, J., & Joshi, M. (2019). RoBERTa: A Robustly Optimized BERT Pretraining Approach. *arXiv preprint arXiv:1907.11692*.\n",
      "5. Dosovitskiy, A., & Brox, T. (2021). Vision Transformers. *arXiv preprint arXiv:2010.11929*.\n",
      "6. Zhang, Y., & Chen, Y. (2022). Efficient Transformers: A Survey. *arXiv preprint arXiv:2205.12345*.\n",
      "7. Radford, A., Wu, J., Child, R., & Luan, D. (2019). Language Models are Unsupervised Multitask Learners. *OpenAI*.\n"
     ]
    }
   ],
   "source": [
    "# Let's now write a final report having incorporated the feedback from the REVIEWER chain\n",
    "\n",
    "FINAL_WRITER_SYS_PROMPT = \"\"\"You are a research assistant and scientific writer.\n",
    "You take in a research report in a set of bullet points with feedback to improve.\n",
    "You revise the research report based on the feedback and write a final version.\"\"\"\n",
    "\n",
    "HUMAN_PROMPT_3 = \"\"\"Write a reviewed and improved version of research report: \n",
    "\n",
    "{input_report}\n",
    "\n",
    "based on this feedback:\n",
    "\n",
    "{input_feedback}\"\"\"\n",
    "\n",
    "final_writer_chat_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", FINAL_WRITER_SYS_PROMPT), \n",
    "    (\"human\", HUMAN_PROMPT_3)\n",
    "])\n",
    "\n",
    "\n",
    "final_writer_chat_model = ChatOpenAI(openai_api_key=OPENAI_API_KEY,\n",
    "                               model=\"gpt-4o-mini\",\n",
    "                               temperature=0.0)\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "final_writer_chain = final_writer_chat_template | final_writer_chat_model | output_parser\n",
    "\n",
    "final_report = final_writer_chain.invoke({\"input_report\": research_report, \"input_feedback\": report_feedback})\n",
    "\n",
    "print(final_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
