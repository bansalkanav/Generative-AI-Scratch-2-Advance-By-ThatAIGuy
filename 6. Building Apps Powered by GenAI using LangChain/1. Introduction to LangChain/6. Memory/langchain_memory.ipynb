{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f5c86a2-76b6-4cf7-8f94-051684e7784d",
   "metadata": {},
   "source": [
    "# **LangChain Memory**\n",
    "\n",
    "Most LLM applications have a conversational interface. An essential component of a conversation is being able to refer to information introduced earlier in the conversation. At bare minimum, a conversational system should be able to access some window of past messages directly. A more complex system will need to have a world model that it is constantly updating, which allows it to do things like maintain information about entities and their relationships.\n",
    "\n",
    "We call this ability to store information about past interactions \"memory\". LangChain provides a lot of utilities for adding memory to a system. These utilities can be used by themselves or incorporated seamlessly into a chain.\n",
    "\n",
    "## **Building memory into a system**\n",
    "The two core design decisions in any memory system are:\n",
    "- How state is stored\n",
    "- How state is queried\n",
    "\n",
    "## **What's covered?**\n",
    "- ConversationBufferMemory\n",
    "- End-to-end Example: Conversational AI Bot\n",
    "- Saving and Loading a Chat History\n",
    "- ChatMessageHistory [Beta]\n",
    "- ConversationBufferWindowMemory [Beta]\n",
    "- ConversationSummaryMemory [Beta]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1113868-bbd5-4d29-a118-7591b61fa8ff",
   "metadata": {},
   "source": [
    "## **ConversationBufferMemory**\n",
    "\n",
    "Let's take a look at how to use ConversationBufferMemory in chains. ConversationBufferMemory is an extremely simple form of memory that just keeps a list of chat messages in a buffer and passes those into the prompt template.\n",
    "\n",
    "This memory type can be connected to a conversation. It allows for storing messages and then extracts the messages in a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37f451bf-b4c1-4f49-afb5-125a09115327",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kanavbansal/Developer/.env_jupyter/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Importing ConversationBufferMemory\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# Set up an instance of Conversation buffer memory\n",
    "memory = ConversationBufferMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0173b80b-ffa0-4763-bec7-613d4c3c3544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': ''}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading Memory\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbe06229-eeda-4386-ae9d-529750380e83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading memory buffer with history\n",
    "\n",
    "memory.buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74df5ae9-8f27-4b80-b69b-6d9190252629",
   "metadata": {},
   "source": [
    "### **Renaming the memory variable and Adding Messages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "165114b6-e0a0-4937-accd-5bf95d0921b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "\n",
    "memory.chat_memory.add_user_message(\"hi!\")\n",
    "memory.chat_memory.add_ai_message(\"what's up?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d2f436c-121c-44a7-94a3-65894bddeea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': \"Human: hi!\\nAI: what's up?\"}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1fc894a-7112-412d-acac-5dd20f90d2b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Human: hi!\\nAI: what's up?\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27dad62-cc81-48bc-aec2-2f32286d73eb",
   "metadata": {},
   "source": [
    "### **By default, memory is returned as a single string. In order to return as a list of messages, you can set `return_messages=True`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64315d88-e7bd-4936-b037-8e94ba227aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "memory.chat_memory.add_user_message(\"hi!\")\n",
    "memory.chat_memory.add_ai_message(\"what's up?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c13ee9e6-4563-4d88-ac57-08ff96c54c3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': [HumanMessage(content='hi!'),\n",
       "  AIMessage(content=\"what's up?\")]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1cf6c5b9-ff8e-4082-9fd7-e1aeba5f49ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='hi!'), AIMessage(content=\"what's up?\")]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1210d13f-efab-42cd-bfa9-3d04865048bb",
   "metadata": {},
   "source": [
    "## **End-to-end Example: Conversational AI Bot**\n",
    "\n",
    "<img src=\"images/memory.png\">\n",
    "\n",
    "### **Steps:**\n",
    "1. Import Chat Model and Configure the API Key\n",
    "2. Create Chat Template\n",
    "3. Initialize the Memory\n",
    "4. Create a Output Parser\n",
    "5. Build a Chain\n",
    "6. Invoke the chain with human_input and chat_history\n",
    "7. Saving to memory\n",
    "8. Run Step 6 and 7 in a loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68d0c98f-d4c6-4a53-89f1-11c67009d285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - Import Chat Model and Configure the API Key\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Setup API Key\n",
    "f = open('keys/.openai_api_key.txt')\n",
    "OPENAI_API_KEY = f.read()\n",
    "\n",
    "# Set the OpenAI Key and initialize a ChatModel\n",
    "chat_model = ChatOpenAI(openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40f452f6-4d40-406e-99fa-414807724238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 - Create Chat Template\n",
    "\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder\n",
    "\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        # The persistent system prompt\n",
    "        SystemMessage(\n",
    "            content=\"You are a chatbot having a conversation with a human.\"\n",
    "        ),\n",
    "        # Creating a chat_history placeholder\n",
    "        MessagesPlaceholder(\n",
    "            variable_name=\"chat_history\"\n",
    "        ),  \n",
    "        # Human Prompt\n",
    "        HumanMessagePromptTemplate.from_template(\n",
    "            \"{human_input}\"\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "298a7a3c-7e07-4e55-b2bf-66241285b153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 - Initialize the Memory\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "824d4ee4-f149-437e-82d5-cc1604e697fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4ff4467-cefe-4dac-9abe-aa1a5dcd86f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a chatbot having a conversation with a human.'), HumanMessage(content='Hi')])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_prompt_template.invoke({\"human_input\": \"Hi\", \"chat_history\": memory.buffer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "306c003b-b708-4abe-9fb4-db825ce3d3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4 - Create a Output Parser\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87955a78-0f81-45df-a31a-37a82396f884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5 - Build a Chain\n",
    "\n",
    "chain = chat_prompt_template | chat_model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c207e06b-8612-4807-a2c1-105f8606dc28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How are you today?'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 6 - Invoke the chain with human_input and chat_history\n",
    "\n",
    "human_input = \"Hi\"\n",
    "\n",
    "response = chain.invoke({\"human_input\": human_input, \"chat_history\": memory.buffer})\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c8d5156-4ef1-47a1-8d51-707e498957e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b812329f-26b6-4e7a-afec-40189215a390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Hi'), AIMessage(content='Hello! How are you today?')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 7 - Saving to Memory\n",
    "\n",
    "memory.chat_memory.add_user_message(human_input)\n",
    "memory.chat_memory.add_ai_message(response)\n",
    "\n",
    "memory.buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb90ceb4-329f-429a-ace5-3893261c45a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your input:  Hi, My name is Kanav.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*User: Hi, My name is Kanav.\n",
      "*AI: Nice to meet you, Kanav! How can I assist you today?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your input:  Can you tell me what kind of tasks can you solve for me?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*User: Can you tell me what kind of tasks can you solve for me?\n",
      "*AI: I can help you with a variety of tasks such as answering questions, providing information on a wide range of topics, offering suggestions or recommendations, assisting with problem-solving, and engaging in casual conversation. Feel free to ask me anything you'd like help with!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your input:  That's good to know.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*User: That's good to know.\n",
      "*AI: I'm glad you think so! If you have any questions or need assistance with anything, feel free to let me know.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your input:  What's my name?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*User: What's my name?\n",
      "*AI: Your name is Kanav.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your input:  bye\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*User: bye\n"
     ]
    }
   ],
   "source": [
    "# Step 8 - Run Step 6 and 7 in a loop\n",
    "\n",
    "while True:\n",
    "    human_input = input('Enter your input: ')\n",
    "    print(f\"*User: {human_input}\")\n",
    "    if human_input in ['bye', 'quit', 'exit']:\n",
    "        break\n",
    "    response = chain.invoke({\"human_input\": human_input, \"chat_history\": memory.buffer})\n",
    "    print(f\"*AI: {response}\")\n",
    "\n",
    "    memory.chat_memory.add_user_message(human_input)\n",
    "    memory.chat_memory.add_ai_message(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46fd022-9da2-4d0b-9581-cf8363f36097",
   "metadata": {},
   "source": [
    "## **Saving a Chat History**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866f57c5-b8f5-4efc-a409-bf4e221629d4",
   "metadata": {},
   "source": [
    "**Let's now learn to save this history on the disk so that whenever we can load the history whenever we chat with our assistant.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8217df9c-fcd3-4f4d-baa2-85e413e7e9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "chat_history = pickle.dumps(memory)\n",
    "\n",
    "with open(\"chats_data/conversation_memory.pkl\", \"wb\") as f:\n",
    "    f.write(chat_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c5c8b1-4521-43ce-83f2-f1c84d3609d0",
   "metadata": {},
   "source": [
    "## **Loading a Chat History**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d9ca65b7-c292-45f2-92e4-20f84679277a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat_memory=InMemoryChatMessageHistory(messages=[HumanMessage(content='Hi'), AIMessage(content='Hello! How are you today?'), HumanMessage(content='Hi, My name is Kanav.'), AIMessage(content='Nice to meet you, Kanav! How can I assist you today?'), HumanMessage(content='Can you tell me what kind of tasks can you solve for me?'), AIMessage(content=\"I can help you with a variety of tasks such as answering questions, providing information on a wide range of topics, offering suggestions or recommendations, assisting with problem-solving, and engaging in casual conversation. Feel free to ask me anything you'd like help with!\"), HumanMessage(content=\"That's good to know.\"), AIMessage(content=\"I'm glad you think so! If you have any questions or need assistance with anything, feel free to let me know.\"), HumanMessage(content=\"What's my name?\"), AIMessage(content='Your name is Kanav.')]) return_messages=True memory_key='chat_history'\n"
     ]
    }
   ],
   "source": [
    "chat_history_loaded = pickle.load(open(\"chats_data/conversation_memory.pkl\", \"rb\"))\n",
    "\n",
    "print(chat_history_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "35fb23db-84d4-45cf-82a9-1e46184fba98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Hi'),\n",
       " AIMessage(content='Hello! How are you today?'),\n",
       " HumanMessage(content='Hi, My name is Kanav.'),\n",
       " AIMessage(content='Nice to meet you, Kanav! How can I assist you today?'),\n",
       " HumanMessage(content='Can you tell me what kind of tasks can you solve for me?'),\n",
       " AIMessage(content=\"I can help you with a variety of tasks such as answering questions, providing information on a wide range of topics, offering suggestions or recommendations, assisting with problem-solving, and engaging in casual conversation. Feel free to ask me anything you'd like help with!\"),\n",
       " HumanMessage(content=\"That's good to know.\"),\n",
       " AIMessage(content=\"I'm glad you think so! If you have any questions or need assistance with anything, feel free to let me know.\"),\n",
       " HumanMessage(content=\"What's my name?\"),\n",
       " AIMessage(content='Your name is Kanav.')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history_loaded.buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d80fdea-b9c7-4b6d-84a6-354df7fa4273",
   "metadata": {},
   "source": [
    "## **ChatMessageHistory**\n",
    "\n",
    "This allows us to mannualy add the messages to memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7c2193d3-ed97-469d-91c6-3d6ce38b255e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "history = ChatMessageHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f937b886-4df3-437f-8138-5d77582322cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "history.add_user_message(\"Hello, Nice to meet you.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "17d20acb-d312-4bdc-9fdd-c5a027e1eff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "history.add_ai_message(\"Nice to meet you!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9530da13-8741-4d78-9660-f429b2b40af8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Hello, Nice to meet you.'),\n",
       " AIMessage(content='Nice to meet you!')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e27849-a745-4de1-8dc9-6e5e0aa07b0a",
   "metadata": {},
   "source": [
    "## **ConversationBufferWindowMemory [Beta]**\n",
    "\n",
    "With this we will have an ability to call back just a window of conversation.\n",
    "\n",
    "`ConversationBufferWindowMemory` keeps a list of the interactions of the conversation over time. It only uses the last K interactions. This can be useful for keeping a sliding window of the most recent interactions, so the buffer does not get too large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d1b54d98-0e59-4814-a056-3e642add1a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the OpenAI Key and initialize a ChatModel\n",
    "chat_model = ChatOpenAI(openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "de991b69-df69-4381-85c6-079ec7a998bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bb5c2798-8e14-4652-ad84-e7ddfb0b15c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI Assistant.\"), \n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{user_input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9a463341-8d6c-499a-8e89-1f00ea172b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up an instance of Conversation buffer window memory\n",
    "window_memory = ConversationBufferWindowMemory(return_messages=True, k=2)\n",
    "# here, k most recent interactions will be refered to answer the user queries\n",
    "\n",
    "# Defining a function to get the messages stored in memory\n",
    "def get_messages_from_memory(user_input):\n",
    "    return window_memory.load_memory_variables(user_input)['chat_history']\n",
    "\n",
    "chain = RunnablePassthrough.assign(\n",
    "            history=RunnableLambda(get_messages_from_memory)\n",
    "        ) | prompt | chat_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42708a72-a593-4116-98cd-f51d87c8a490",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91922a38-a61c-47da-94df-cb7a76726023",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5e3eb121-e9ce-47c9-bf4e-f0890eff7fe6",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'chat_history'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m input_prompt \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_input\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat are the three biggest states in India by population?\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m----> 3\u001b[0m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_prompt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/.env_jupyter/lib/python3.9/site-packages/langchain_core/runnables/base.py:2873\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   2869\u001b[0m config \u001b[38;5;241m=\u001b[39m patch_config(\n\u001b[1;32m   2870\u001b[0m     config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2871\u001b[0m )\n\u001b[1;32m   2872\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2873\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2874\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2875\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m~/Developer/.env_jupyter/lib/python3.9/site-packages/langchain_core/runnables/passthrough.py:495\u001b[0m, in \u001b[0;36mRunnableAssign.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    490\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    491\u001b[0m     \u001b[38;5;28minput\u001b[39m: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[1;32m    492\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    493\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    494\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[0;32m--> 495\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_invoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/.env_jupyter/lib/python3.9/site-packages/langchain_core/runnables/base.py:1784\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[0;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   1780\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[1;32m   1781\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, child_config)\n\u001b[1;32m   1782\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[1;32m   1783\u001b[0m         Output,\n\u001b[0;32m-> 1784\u001b[0m         \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1785\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1786\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1787\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1788\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1789\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1790\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1791\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1792\u001b[0m     )\n\u001b[1;32m   1793\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1794\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/Developer/.env_jupyter/lib/python3.9/site-packages/langchain_core/runnables/config.py:404\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    403\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[0;32m--> 404\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/.env_jupyter/lib/python3.9/site-packages/langchain_core/runnables/passthrough.py:482\u001b[0m, in \u001b[0;36mRunnableAssign._invoke\u001b[0;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_invoke\u001b[39m(\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;28minput\u001b[39m: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    475\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m    476\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    477\u001b[0m         \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mdict\u001b[39m\n\u001b[1;32m    478\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe input to RunnablePassthrough.assign() must be a dict.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    480\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    481\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m--> 482\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    487\u001b[0m     }\n",
      "File \u001b[0;32m~/Developer/.env_jupyter/lib/python3.9/site-packages/langchain_core/runnables/base.py:3562\u001b[0m, in \u001b[0;36mRunnableParallel.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   3549\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m get_executor_for_config(config) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[1;32m   3550\u001b[0m         futures \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   3551\u001b[0m             executor\u001b[38;5;241m.\u001b[39msubmit(\n\u001b[1;32m   3552\u001b[0m                 step\u001b[38;5;241m.\u001b[39minvoke,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3560\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key, step \u001b[38;5;129;01min\u001b[39;00m steps\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   3561\u001b[0m         ]\n\u001b[0;32m-> 3562\u001b[0m         output \u001b[38;5;241m=\u001b[39m {key: future\u001b[38;5;241m.\u001b[39mresult() \u001b[38;5;28;01mfor\u001b[39;00m key, future \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(steps, futures)}\n\u001b[1;32m   3563\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   3564\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Developer/.env_jupyter/lib/python3.9/site-packages/langchain_core/runnables/base.py:3562\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   3549\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m get_executor_for_config(config) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[1;32m   3550\u001b[0m         futures \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   3551\u001b[0m             executor\u001b[38;5;241m.\u001b[39msubmit(\n\u001b[1;32m   3552\u001b[0m                 step\u001b[38;5;241m.\u001b[39minvoke,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3560\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key, step \u001b[38;5;129;01min\u001b[39;00m steps\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   3561\u001b[0m         ]\n\u001b[0;32m-> 3562\u001b[0m         output \u001b[38;5;241m=\u001b[39m {key: \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m key, future \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(steps, futures)}\n\u001b[1;32m   3563\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   3564\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/concurrent/futures/_base.py:438\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 438\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/concurrent/futures/_base.py:390\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 390\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    392\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    393\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/concurrent/futures/thread.py:52\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m~/Developer/.env_jupyter/lib/python3.9/site-packages/langchain_core/runnables/base.py:4441\u001b[0m, in \u001b[0;36mRunnableLambda.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4427\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Invoke this Runnable synchronously.\u001b[39;00m\n\u001b[1;32m   4428\u001b[0m \n\u001b[1;32m   4429\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4438\u001b[0m \u001b[38;5;124;03m    TypeError: If the Runnable is a coroutine function.\u001b[39;00m\n\u001b[1;32m   4439\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4440\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunc\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 4441\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4442\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_invoke\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4443\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4444\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4445\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4446\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4447\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4448\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   4449\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot invoke a coroutine function synchronously.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4450\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse `ainvoke` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4451\u001b[0m     )\n",
      "File \u001b[0;32m~/Developer/.env_jupyter/lib/python3.9/site-packages/langchain_core/runnables/base.py:1784\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[0;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   1780\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[1;32m   1781\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, child_config)\n\u001b[1;32m   1782\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[1;32m   1783\u001b[0m         Output,\n\u001b[0;32m-> 1784\u001b[0m         \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1785\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1786\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1787\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1788\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1789\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1790\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1791\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1792\u001b[0m     )\n\u001b[1;32m   1793\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1794\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/Developer/.env_jupyter/lib/python3.9/site-packages/langchain_core/runnables/config.py:404\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    403\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[0;32m--> 404\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/.env_jupyter/lib/python3.9/site-packages/langchain_core/runnables/base.py:4297\u001b[0m, in \u001b[0;36mRunnableLambda._invoke\u001b[0;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[1;32m   4295\u001b[0m                 output \u001b[38;5;241m=\u001b[39m chunk\n\u001b[1;32m   4296\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4297\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4298\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m   4299\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4300\u001b[0m \u001b[38;5;66;03m# If the output is a Runnable, invoke it\u001b[39;00m\n\u001b[1;32m   4301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, Runnable):\n",
      "File \u001b[0;32m~/Developer/.env_jupyter/lib/python3.9/site-packages/langchain_core/runnables/config.py:404\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    403\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[0;32m--> 404\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[44], line 7\u001b[0m, in \u001b[0;36mget_messages_from_memory\u001b[0;34m(user_input)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_messages_from_memory\u001b[39m(user_input):\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwindow_memory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_memory_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mchat_history\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'chat_history'"
     ]
    }
   ],
   "source": [
    "input_prompt = {\"user_input\": \"What are the three biggest states in India by population?\"}\n",
    "\n",
    "chain.invoke(input_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "86a6f9c7-9877-4114-a8e4-792c739c6db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hello, How are you?\n",
      "AI: Hello! I'm doing well, thank you for asking. I've been busy processing information and learning new things. How about you?\n",
      "Human: What is the new thing you learned recently?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I recently learned about a new algorithm for image recognition that improves accuracy by 10%. It's called Convolutional Neural Networks with Attention Mechanism (CNN-AM). This algorithm focuses on specific parts of an image to make more accurate predictions. I found it fascinating how it can significantly improve image recognition tasks.\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is the new thing you learned recently?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6957c84a-f049-486b-8357-7e942285d36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: What is the new thing you learned recently?\n",
      "AI: I recently learned about a new algorithm for image recognition that improves accuracy by 10%. It's called Convolutional Neural Networks with Attention Mechanism (CNN-AM). This algorithm focuses on specific parts of an image to make more accurate predictions. I found it fascinating how it can significantly improve image recognition tasks.\n",
      "Human: Sounds interesting. Can you teach it to me in a simple term?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Sure! In simple terms, the CNN-AM algorithm is like a detective that pays extra attention to important parts of a picture to figure out what the picture is showing. It helps computers recognize images more accurately by focusing on key details.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Sounds interesting. Can you teach it to me in a simple term?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "72908c73-13cc-4e00-a47d-f39165b390f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat_memory=ChatMessageHistory(messages=[HumanMessage(content='Hello, How are you?'), AIMessage(content=\"Hello! I'm doing well, thank you for asking. I've been busy processing information and learning new things. How about you?\"), HumanMessage(content='What is the new thing you learned recently?'), AIMessage(content=\"I recently learned about a new algorithm for image recognition that improves accuracy by 10%. It's called Convolutional Neural Networks with Attention Mechanism (CNN-AM). This algorithm focuses on specific parts of an image to make more accurate predictions. I found it fascinating how it can significantly improve image recognition tasks.\"), HumanMessage(content='Sounds interesting. Can you teach it to me in a simple term?'), AIMessage(content='Sure! In simple terms, the CNN-AM algorithm is like a detective that pays extra attention to important parts of a picture to figure out what the picture is showing. It helps computers recognize images more accurately by focusing on key details.')]) k=1\n"
     ]
    }
   ],
   "source": [
    "print(conversation.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1d247501-b4c9-4217-9625-52a9013bc39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Sounds interesting. Can you teach it to me in a simple term?\n",
      "AI: Sure! In simple terms, the CNN-AM algorithm is like a detective that pays extra attention to important parts of a picture to figure out what the picture is showing. It helps computers recognize images more accurately by focusing on key details.\n"
     ]
    }
   ],
   "source": [
    "print(window_memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e96e65-92d9-4586-9e87-2a867905de1a",
   "metadata": {},
   "source": [
    "## **ConversationSummaryBufferMemory [Beta]**\n",
    "\n",
    "It allows us to call a LLM for a summary of all the historical conversation.\n",
    "\n",
    "`ConversationSummaryBufferMemory` combines the two ideas. It keeps a buffer of recent interactions in memory, but rather than just completely flushing old interactions it compiles them into a summary and uses both. It uses token length rather than number of interactions to determine when to flush interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "84384898-a0e2-47bd-a657-5a6e04f8afb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "summary_memory = ConversationSummaryBufferMemory(llm=chat_model, max_token_limit=100)\n",
    "\n",
    "conversation = ConversationChain(llm=chat_model, memory=summary_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "28737e5c-ead4-4d73-96ad-3ad24b40e764",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Sure! For your upcoming Data Science interview, I recommend starting with the basics of Python programming. Here's a suggested study plan:\\n\\n1. Week 1: Introduction to Python - Learn about Python syntax, data types, variables, and basic operations.\\n2. Week 2: Control flow and functions - Dive into if statements, loops, and functions in Python.\\n3. Week 3: Data structures - Explore lists, dictionaries, tuples, and sets in Python.\\n4. Week 4: File handling and modules - Understand how to read and write files, as well as import modules in Python.\\n5. Week 5: Object-oriented programming - Learn about classes, objects, and inheritance in Python.\\n6. Week 6: Libraries and frameworks - Familiarize yourself with popular libraries like NumPy, Pandas, and Matplotlib for data manipulation and visualization.\\n7. Week 7: Practice coding exercises - Solve coding challenges on platforms like LeetCode or HackerRank to reinforce your skills.\\n8. Week 8: Review and mock interviews - Review key concepts and participate in mock interviews to prepare for your Data Science interview.\\n\\nRemember to practice regularly and apply what you've learned to real-world projects. Good luck with your interview preparation! Let me know if you have any other questions.\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Give me a study plan for Python Programming for my upcoming Data Science Interview.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bf2997a5-a9ba-48e3-8fb6-19cdd0a5fb12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"For Machine Learning, I recommend a 12-week study plan to cover the fundamentals of machine learning algorithms, data preprocessing, model evaluation, feature selection, and tuning hyperparameters. I also suggest diving into specific topics such as regression, classification, clustering, and neural networks. Additionally, it's important to work on real-world projects and participate in Kaggle competitions to apply your knowledge and showcase your skills to potential employers during the interview process. Let me know if you need more specific details or resources for your study plan!\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Also share for Machine Learning.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2f1ed8b0-782b-420c-8725-77d1beb5df42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(summary_memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3711d1f2-ee16-412a-a694-c9d687e7fe4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x0000024EA67AC2B0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x0000024EA67AF7C0>, openai_api_key=SecretStr('**********'), openai_proxy='') max_token_limit=100 moving_summary_buffer='The human asks the AI for study plans for Python Programming and Machine Learning for an upcoming Data Science Interview. The AI provides a detailed 8-week study plan for Python and a 12-week study plan for Machine Learning, emphasizing the importance of regular practice, real-world application, and participating in Kaggle competitions to showcase skills.'\n"
     ]
    }
   ],
   "source": [
    "print(conversation.memory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
