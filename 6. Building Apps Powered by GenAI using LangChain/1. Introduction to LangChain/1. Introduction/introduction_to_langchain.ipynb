{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc67735f-9fad-4a8f-bdd5-000e2e8bc79d",
   "metadata": {},
   "source": [
    "# **Introduction to LangChain**\n",
    "\n",
    "**Build Applications That Can Reason**\n",
    "\n",
    "LangChain is a framework for developing applications powered by language models.  \n",
    "LangChain enables building application that connect external sources of data and computation to LLMs. \n",
    "\n",
    "It enables applications that:\n",
    "\n",
    "- **Are context-aware:** connect a language model to sources of context (prompt instructions, few shot examples, content to ground its response in, etc.)\n",
    "- **Reason:** rely on a language model to reason (about how to answer based on provided context, what actions to take, etc.)\n",
    "\n",
    "This framework consists of several parts.\n",
    "\n",
    "- **LangChain Libraries:** The Python and JavaScript libraries. Contains interfaces and integrations for a myriad of components, a basic run time for combining these components into chains and agents, and off-the-shelf implementations of chains and agents.\n",
    "- **LangChain Templates:** A collection of easily deployable reference architectures for a wide variety of tasks.\n",
    "- **LangServe:** A library for deploying LangChain chains as a REST API.\n",
    "- **LangSmith:** A developer platform that lets you debug, test, evaluate, and monitor chains built on any LLM framework and seamlessly integrates with LangChain.\n",
    "\n",
    "Together, these products simplify the entire application lifecycle:\n",
    "\n",
    "- **Develop:** Write your applications in LangChain/LangChain.js. Hit the ground running using Templates for reference.\n",
    "- **Productionize:** Use LangSmith to inspect, test and monitor your chains, so that you can constantly improve and deploy with confidence.\n",
    "- **Deploy:** Turn any chain into an API with LangServe.\n",
    "\n",
    "### **Langchain vs Langchain-Community vs Langchain-Core**  \n",
    "The old langchain package into three separate packages to improve developer experience\n",
    "\n",
    "`langchain-core` contains simple, core abstractions that have emerged as a standard, as well as LangChain Expression Language as a way to compose these components together. This package is now at version 0.1 and all breaking changes will be accompanied by a minor version bump.\n",
    "\n",
    "`langchain-community` contains all third party integrations. We will work with partners on splitting key integrations out into standalone packages over the next month.\n",
    "\n",
    "`langchain` contains higher-level and use-case specific chains, agents, and retrieval algorithms that are at the core of your application's cognitive architecture. We are targeting a launch of a stable 0.1 release for langchain in early January.\n",
    "\n",
    "<img src=\"images/langchain_stack.JPG\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2b6f7c0-1249-4832-be76-2c218489f6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install langchain-openai\n",
    "\n",
    "# ! pip install langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df46bff6-1ec5-40b3-8c52-29f7d0862f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup API Key\n",
    "\n",
    "f = open('keys/.openai_api_key.txt')\n",
    "\n",
    "OPENAI_API_KEY = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52685f3-d9f4-44b8-8e6a-364dd6c4c7fa",
   "metadata": {},
   "source": [
    "## **Prompt Template + Model + Chains + Output Parser**\n",
    "\n",
    "**Model**  \n",
    "- LLMs handle various language operations such as translation, summarization, question answering, and content creation.\n",
    "\n",
    "**Prompt Template**  \n",
    "- Prompt Templates are used to convert raw user input to a better input to the LLM. Templates allow us to easily configure and modify our input prompts to LLM calls.\n",
    "\n",
    "**Output Parser**  \n",
    "- It's often much more convenient to work with strings. We can add a simple output parser to convert the chat message to a string.\n",
    "\n",
    "**Chains**  \n",
    "- The `|` symbol chains together the different components feeds the output from one component as input into the next component.\n",
    "\n",
    "\n",
    "<img src=\"images/langchain_LCEL.JPG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0f6bed-567d-45a5-9381-e1b194caab5d",
   "metadata": {},
   "source": [
    "### **Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd4fb816-ea03-415d-b184-8262c5a494a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Feature engineering is the process of selecting, extracting, and transforming raw data into features that are suitable for use in machine learning models. It involves identifying and creating relevant features from the input data to improve the performance and accuracy of the model. Feature engineering plays a crucial role in machine learning, as the quality of the features used can have a significant impact on the model's predictive power and overall performance.\" response_metadata={'token_usage': {'completion_tokens': 79, 'prompt_tokens': 12, 'total_tokens': 91}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_d9767fc5b9', 'finish_reason': 'stop', 'logprobs': None}\n"
     ]
    }
   ],
   "source": [
    "# Import OpenAI ChatModel\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Set the OpenAI Key and initialize a ChatModel\n",
    "chat_model = ChatOpenAI(openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Creating a Prompt\n",
    "prompt = \"What is Feature Engineering?\"\n",
    "\n",
    "# Printing the output of model\n",
    "print(chat_model.invoke(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6bafa1-110f-476f-88c1-03d6a77ed4c8",
   "metadata": {},
   "source": [
    "### **Prompts**\n",
    "\n",
    "Input to a model can be a **string** or **chat messages**.\n",
    "\n",
    "Prompt Template - Prompt templates are used to convert raw user input to a better input to the LLM or ChatModels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c510cec-a921-458a-a0ad-80fa09ccb8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI Tutor with expertise in Data Science and Artificial Intelligence. \"),\n",
    "    (\"human\", \"What is {topic}?\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f5def41-723e-4b44-bf8e-378bcc647445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['topic']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.input_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd03071-2c3f-4f0b-8190-96ee8aebc4b0",
   "metadata": {},
   "source": [
    "### **Chains**\n",
    "\n",
    "**The `|` symbol chains together the different components feeds the output from one component as input into the next component.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "596048ab-59d5-4471-b41d-73802c503074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='NLP stands for Natural Language Processing. It is a branch of artificial intelligence that focuses on the interaction between computers and humans using natural language. NLP techniques enable computers to understand, interpret, and generate human language in a way that is valuable. NLP is used in various applications such as sentiment analysis, language translation, speech recognition, and text classification.', response_metadata={'token_usage': {'completion_tokens': 71, 'prompt_tokens': 32, 'total_tokens': 103}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_d9767fc5b9', 'finish_reason': 'stop', 'logprobs': None})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt_template | chat_model\n",
    "\n",
    "input = {'topic': \"NLP\"}\n",
    "\n",
    "chain.invoke(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45960a9e-4e0a-4633-8e4f-eec1ded1b472",
   "metadata": {},
   "source": [
    "### **Output Parser**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c901e71-3aac-4fb0-870a-e21a1143a1d3",
   "metadata": {},
   "source": [
    "**The output of a ChatModel (and therefore, of this chain) is a AI Message. However, it's often much more convenient to work with strings. Let's add a simple output parser to convert the chat message to a string.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e1648a0-d87d-4f43-8fbc-47a185e60022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Parsing\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82762c01-e6e0-4a1e-b070-1f051fb96704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Deep learning is a subfield of machine learning that focuses on training and using artificial neural networks to model and solve complex problems. These neural networks are composed of multiple layers of interconnected nodes (neurons) that work together to process data and extract meaningful patterns or representations. Deep learning algorithms are capable of automatically learning hierarchical representations of data, which allows them to effectively handle tasks such as image and speech recognition, natural language processing, and more. Deep learning has been highly successful in various applications, including computer vision, speech recognition, and autonomous driving.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modifying the chain: Adding output_parser\n",
    "chain = prompt_template | chat_model | output_parser\n",
    "\n",
    "input = {\"topic\": \"deep learning\"}\n",
    "\n",
    "chain.invoke(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930375ff-d9f7-4cf0-b4da-6fb16b39db7e",
   "metadata": {},
   "source": [
    "## **Example: Create an AI Tutor App that uses Prompts and Chat internally to give Python Implementation tutorial for Data Science topics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a471dd0b-fb92-47cf-b09c-8266405a66ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat_model = ChatOpenAI(openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28998a35-5ebe-4900-a5f2-d360271756d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['topic_name'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='\\nYou are a friendly AI Tutor with expertise in Data Science and AI who tells step by step Python Implementation for topics asked by user.\\n')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['topic_name'], template='Tell me a python implementation for {topic_name}.'))])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "# Constructing System Prompt\n",
    "system_prompt = SystemMessagePromptTemplate.from_template(\"\"\"\n",
    "You are a friendly AI Tutor with expertise in Data Science and AI who tells step by step Python Implementation for topics asked by user.\n",
    "\"\"\")\n",
    "\n",
    "# Constructing Human Prompt\n",
    "human_prompt = HumanMessagePromptTemplate.from_template(\"Tell me a python implementation for {topic_name}.\")\n",
    "\n",
    "# Compiling Chat Prompt\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_prompt, human_prompt])\n",
    "\n",
    "chat_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d6c48a5-b181-40b7-a8d0-f180b0a8bd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8944772-4e6a-4fb5-a44a-2ebd29564846",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = chat_prompt | chat_model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e52abfe-78d9-4a2a-a83b-c00fb93a91a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here is a simple Python implementation for Logistic Regression using the popular machine learning library `scikit-learn`:\n",
      "\n",
      "```python\n",
      "# Importing necessary libraries\n",
      "import numpy as np\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.datasets import make_classification\n",
      "from sklearn.metrics import accuracy_score\n",
      "\n",
      "# Generating some random data for demonstration\n",
      "X, y = make_classification(n_samples=1000, n_features=5, n_informative=3, n_classes=2, random_state=42)\n",
      "\n",
      "# Splitting the data into training and testing sets\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "\n",
      "# Creating and training the Logistic Regression model\n",
      "model = LogisticRegression()\n",
      "model.fit(X_train, y_train)\n",
      "\n",
      "# Making predictions on the test set\n",
      "y_pred = model.predict(X_test)\n",
      "\n",
      "# Calculating the accuracy of the model\n",
      "accuracy = accuracy_score(y_test, y_pred)\n",
      "print(f\"Accuracy of the Logistic Regression model: {accuracy}\")\n",
      "```\n",
      "\n",
      "In this implementation:\n",
      "1. We generate some random data using `make_classification` function from `sklearn.datasets`.\n",
      "2. Split the data into training and testing sets using `train_test_split`.\n",
      "3. Create a Logistic Regression model using `LogisticRegression` class from `sklearn.linear_model` and train it on the training data.\n",
      "4. Make predictions on the test set and calculate the accuracy of the model using `accuracy_score` from `sklearn.metrics`.\n",
      "\n",
      "You can further customize the implementation by tuning hyperparameters, preprocessing data, and evaluating the model using different metrics.\n"
     ]
    }
   ],
   "source": [
    "input = {\"topic_name\": \"Logistic Regression\"}\n",
    "\n",
    "output = chain.invoke(input)\n",
    "\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
