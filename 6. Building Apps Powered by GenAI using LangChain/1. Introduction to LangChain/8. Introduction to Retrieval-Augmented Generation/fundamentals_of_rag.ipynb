{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d96fdaef-df89-4c0a-b297-046e5232671e",
   "metadata": {},
   "source": [
    "# **Introduction to Retrieval Augmented Generation**\n",
    "\n",
    "We know that LLMs have the capability to generate stuff by themselves. But these tools aren't perfect.\n",
    "\n",
    "Even though they're super smart, they sometimes get things wrong, especially if they need to be really precise or use the latest information. So, to fix this, some of the brightest minds at Meta AI came up with a new trick called retrieval-augmented generation, or RAG for short, in 2020.\n",
    "\n",
    "Think of it as giving our language models an assistant. This assistant digs through a massive pile of updated information and feeds the most relevant and recent bits to the LLM.\n",
    "\n",
    "**Benefits:**  \n",
    "1. **Enhanced factual accuracy and Domain Specific Expertise:** Imagine a customer service chatbot trained on general conversation data. It might struggle with technical domain specific questions. RAGs allow you to integrate domain-specific knowledge bases, enabling the chatbot to handle these inquires with expertise.\n",
    "2. **Reduce Hallucination:** LLMs can generate false information, a phenomenon known as hallucination. The knowledge base provided can help support the claims of generative model.\n",
    "\n",
    "**Components of RAG:**  \n",
    "1. **Retrieval:** When a user asks a question or provides a prompt, retrievals first help fetch relevant passages from a vast knowledge base. This Knowledge Base could be the company's internal documents, or any other source of text data.\n",
    "2. **Augmentation:** The retrieved passages are then used to \"augment\" the LLM's knowledge. This can include various techniques, such as summarization or encoding the key information.\n",
    "3. **Generation:** Finally LLM leverages its understanding of language along with the augmented information to generate a response. This response can be an answer to a question, a creative text format based on a prompt, etc...\n",
    "\n",
    "**Applications:**  \n",
    "1. Question Answering: A RAG powered customer care chatbot can answer customer queries by retrieving product information, FAQs and guides to provide a well-rounded response.\n",
    "2. Document Summarization: A research paper summarization tool can use RAG to retrieve relevant sections and then generate a summary highlighting main points.\n",
    "3. Creative Text Generation: A story writing assistant can use RAG to retrieve information about historical periods or fictional creation, helping LLM to generate more deeply engaging stories.\n",
    "4. Code Generation: A code completion tool can use RAG to retrieve relevant code examples and API documentation, helping developers write code more efficiently.\n",
    "\n",
    "\n",
    "## **What are Retrievals?**\n",
    "Understand that the retrievals are specialized in navigating through vast amounts of data to find information that is relevant to a specific query or context. \n",
    "\n",
    "Retrieval models focus on the precision of matching query criteria with the data they have access to. Note that retrieval models rely heavily on the quality and structure of the data they access. Their performance depends on the relevance and accuracy of the information stored in the databases they query. \n",
    "\n",
    "In simple terms, retrievals search and identify relevant data from a large corpus for a given query.\n",
    "\n",
    "## **Building a RAG System**\n",
    "Step 1: Create an Index on available Knowledge Base  \n",
    "- Data from formats like PDF, HTML, etc is cleaned and converted into plain text. This text is then divided into smaller parts (i.e chunks) and turned into vector representations by passing the chunks into the embedding model to make it easier to find later.\n",
    "\n",
    "Step 2: Create a Retrival\n",
    "- When someone asks a question, the RAG system turns that question into vector embedding using the same method used in indexing. Then, it compares this vector to the vectors of the indexed text parts to fing the `k` most similar chunks. These `k` most similar chunks are used in the next step as a context.\n",
    "\n",
    "Step 3: Generation  \n",
    "- The system combines the retrieved text parts (i.e. context) with the original question to create a prompt. The LLM uses this prompt to answer the question.\n",
    "\n",
    "**Step 1: Create an Index on available Knowledge Base**      \n",
    "1. **Data Collection:** Carefully ingest the data from various sources. This data forms the basis of Knowledge Base.\n",
    "2. **Split and Parse:** Once the data is ingested, it needs to be broken down into manageable chunks. This is important because the LLMs havve a maximum context wondow that they can process in one go. During this step the data is not only splitted but also parsed to extract the useful metadata. Metadata can be information like document title, authors, etc...\n",
    "3. **Embedding Generation:** The next step is to convert the chunks into vector embeddings. This can be done using embedding models like BERT, GPT etc... that transforms text into a vector space while capturing semantic relationships and contextual meaning into numberical representation.\n",
    "4. **Vector Database:** The final step is to store the generated embeddings along with the metadata in a vector database such as ChromaDB, PineCone, etc... These databases are optimized for handing large volumes of data and allow efficient querying."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a066b8-0833-4ea1-9176-c8b249de4119",
   "metadata": {},
   "source": [
    "## **Step 1: Create an Index on available Knowledge Base**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a89c3c0-5275-4e16-ab26-ad2e22956447",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 10/10 [00:00<00:00, 5207.73it/s]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = DirectoryLoader('data/subtitles', glob=\"*.srt\", show_progress=True, loader_cls=TextLoader)\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "747de193-c8a0-4d94-9717-e34654032b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "chunks = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b0cbd48-dc32-49a7-b553-401e69dac7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Documents: 10\n",
      "\n",
      "Number of Chunks: 514\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Documents:\", len(docs))\n",
    "print()\n",
    "print(\"Number of Chunks:\", len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "281e29a4-8011-43cc-b35d-313650acc5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "f = open(\"keys/.openai_api_key.txt\")\n",
    "OPENAI_API_KEY = f.read()\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32559d13-b90e-432a-b054-0d4f77afa671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install langchain_chroma\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "CHROMA_PATH = \"./chroma_db_\"\n",
    "\n",
    "db_chroma = Chroma.from_documents(chunks, embeddings_model, persist_directory=CHROMA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6e98874-6698-4113-b9fe-adbe91815856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up a DB connection \n",
    "\n",
    "db_chroma = Chroma(persist_directory=CHROMA_PATH, embedding_function=embeddings_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7700114-cf98-4f39-8481-8d4c735f4567",
   "metadata": {},
   "source": [
    "## **Step 2: Create a Retrieval**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "07b3ecfb-a9d7-4a36-9631-089586c4ae41",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Who is Rachem?'\n",
    "\n",
    "docs_chroma = db_chroma.similarity_search_with_score(query, k=5)\n",
    "\n",
    "context_text = \"\\n\\n\".join([doc.page_content for doc, _score in docs_chroma])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e582cb8a-3304-460f-9e27-c137722c264d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs_chroma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cfe7b987-4e8b-45e4-8db0-c39b17d35430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'242\\n00:14:53,433 --> 00:14:55,264\\n\"Just a waitress\"?\\n\\n243\\n00:14:56,569 --> 00:14:58,298\\nNow that was....\\n\\n244\\n00:14:58,738 --> 00:15:00,865\\nI mean, as opposed to....\\n\\n245\\n00:15:02,375 --> 00:15:04,707\\nOkay, is this over yet? Rach?\\n\\n246\\n00:15:05,478 --> 00:15:08,879\\nI do not have chubby ankles!\\n\\n247\\n00:15:09,082 --> 00:15:10,242\\nNo! I\\n\\n248\\n00:15:10,483 --> 00:15:13,611\\nOkay, look at the other side.\\nLook at Julie\\'s column.\\n\\n249\\n00:15:14,487 --> 00:15:15,954\\n\"She\\'s not Rachem\"?\\n\\n242\\n00:14:53,433 --> 00:14:55,264\\n\"Just a waitress\"?\\n\\n243\\n00:14:56,569 --> 00:14:58,298\\nNow that was....\\n\\n244\\n00:14:58,738 --> 00:15:00,865\\nI mean, as opposed to....\\n\\n245\\n00:15:02,375 --> 00:15:04,707\\nOkay, is this over yet? Rach?\\n\\n246\\n00:15:05,478 --> 00:15:08,879\\nI do not have chubby ankles!\\n\\n247\\n00:15:09,082 --> 00:15:10,242\\nNo! I\\n\\n248\\n00:15:10,483 --> 00:15:13,611\\nOkay, look at the other side.\\nLook at Julie\\'s column.\\n\\n249\\n00:15:14,487 --> 00:15:15,954\\n\"She\\'s not Rachem\"?\\n\\n46\\n00:03:06,910 --> 00:03:11,010\\nI\\'m the same way.Show me a bottle of wine.\\nand I\\'m like, Wow, who am I?\\n\\n47\\n00:03:13,254 --> 00:03:16,553\\nWait. We\\'re talking about Rachel.\\nYou and Rachel.\\n\\n48\\n00:03:16,758 --> 00:03:19,727\\nI\\'ve been dreaming about\\nme and Rachel for 10 years.\\n\\n49\\n00:03:19,927 --> 00:03:21,656\\nBut now I\\'m with Julie.\\n\\n50\\n00:03:21,896 --> 00:03:24,763\\nSo it\\'s like, me and Julie,\\nme and Rachel.\\n\\n51\\n00:03:24,999 --> 00:03:27,092\\nMe and Julie, me and Rach\\n\\n46\\n00:03:06,910 --> 00:03:11,010\\nI\\'m the same way.Show me a bottle of wine.\\nand I\\'m like, Wow, who am I?\\n\\n47\\n00:03:13,254 --> 00:03:16,553\\nWait. We\\'re talking about Rachel.\\nYou and Rachel.\\n\\n48\\n00:03:16,758 --> 00:03:19,727\\nI\\'ve been dreaming about\\nme and Rachel for 10 years.\\n\\n49\\n00:03:19,927 --> 00:03:21,656\\nBut now I\\'m with Julie.\\n\\n50\\n00:03:21,896 --> 00:03:24,763\\nSo it\\'s like, me and Julie,\\nme and Rachel.\\n\\n51\\n00:03:24,999 --> 00:03:27,092\\nMe and Julie, me and Rach\\n\\n190\\n00:11:29,896 --> 00:11:31,090\\nWell....\\n\\n191\\n00:11:31,364 --> 00:11:32,524\\nHe broke up with Julie!\\n\\n192\\n00:11:34,200 --> 00:11:37,328\\nWell, go hug her, for God\\'s sakes!\\n\\n193\\n00:11:38,738 --> 00:11:39,705\\nReally?\\n\\n194\\n00:11:39,906 --> 00:11:40,838\\nReally.\\n\\n195\\n00:11:41,941 --> 00:11:43,841\\nIt\\'s always been you, Rach.\\n\\n196\\n00:11:44,077 --> 00:11:45,738\\nOh, God.\\n\\n197\\n00:11:57,424 --> 00:11:59,016\\nOh, this is good.\\n\\n198\\n00:11:59,225 --> 00:12:01,284\\n-This is really good.\\n-I know. I know.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5c477a-c105-4856-bb50-94f00a81549c",
   "metadata": {},
   "source": [
    "## **Step 3: Generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2d52aef9-dbb6-44e4-aeb3-f65c189a9ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question based only on the following context:\n",
    "{context}\n",
    "Answer the question based on the above context: {question}.\n",
    "Provide a detailed answer.\n",
    "Don’t justify your answers.\n",
    "Don’t give information not mentioned in the CONTEXT INFORMATION.\n",
    "Do not say \"according to the context\" or \"mentioned in the context\" or similar.\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "\n",
    "prompt = prompt_template.format(context=context_text, question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b10b103c-b123-4192-9c2e-37ee8791cdd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rachem is a typo or mispronunciation of the name Rachel, as seen in the dialogue where the characters mention \"She's not Rachem\" and \"It's always been you, Rach.\"\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "response_text = model.invoke(prompt)\n",
    "\n",
    "print(response_text.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
