{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b355560-7854-4da4-8340-74e406194393",
   "metadata": {},
   "source": [
    "# **Text Transformation using Text Splitters**\n",
    "\n",
    "Notice that after you load a Documnet object from a source, you end up with strings by grabbing them from page_content. In certain situations, the length of the strings may be too large to feed into a model, both embedding and chat model.\n",
    "\n",
    "**Note: LLMs have fixed maximum context window. If you document contains more token then the maximum context window size, LLMs won't be able to process it. So, it is important to break the documents into chunks.**\n",
    "\n",
    "Once you've loaded documents, you'll often want to transform them to better suit your application. The simplest example is you may want to split a long document into smaller chunks that can fit into your model's context window. LangChain has a number of built-in document transformers that make it easy to split, combine, filter, and otherwise manipulate documents.\n",
    "\n",
    "When you want to deal with long pieces of text, it is necessary to split up that text into chunks. As simple as this sounds, there is a lot of potential complexity here. Ideally, you want to keep the semantically related pieces of text together. What \"semantically related\" means could depend on the type of text. This notebook showcases several ways to do that.\n",
    "\n",
    "At a high level, text splitters work as following:\n",
    "\n",
    "1. Split the text up into small, semantically meaningful chunks (often sentences).\n",
    "2. Start combining these small chunks into a larger chunk until you reach a certain size (as measured by some function).\n",
    "3. Once you reach that size, make that chunk its own piece of text and then start creating a new chunk of text with some overlap (to keep context between chunks).\n",
    "\n",
    "That means there are two different axes along which you can customize your text splitter:\n",
    "\n",
    "1. How the text is split\n",
    "2. How the chunk size is measured\n",
    "\n",
    "## **Types of Text Splitters**\n",
    "\n",
    "**Evaluate text splitters**  \n",
    "You can evaluate text splitters with the [Chunkviz utility](https://www.chunkviz.com/) created by Greg Kamradt. Chunkviz is a great tool for visualizing how your text splitter is working. It will show you how your text is being split up and help in tuning up the splitting parameters.\n",
    "\n",
    "LangChain offers many different types of text splitters. These all live in the `langchain-text-splitters` package. Below is a table listing all of them, along with a few characteristics:\n",
    "> **Name:** Name of the text splitter  \n",
    "> **Splits On:** How this text splitter splits text  \n",
    "> **Adds Metadata:** Whether or not this text splitter adds metadata about where each chunk came from.  \n",
    "> **Description:** Description of the splitter, including recommendation on when to use it.\n",
    "\n",
    "| Name | Splits On | Adds Metadata | Description |\n",
    "| :--- | :--- | :---: | :--- |\n",
    "| Character | A user defined character | . | Splits text based on a user defined character. One of the simpler methods. |\n",
    "| Recursive | A list of user defined characters | . | Recursively splits text. Splitting text recursively serves the purpose of trying to keep related pieces of text next to each other. This is the recommended way to start splitting text. |\n",
    "| HTML | HTML specific characters | ✅ | Splits text based on HTML-specific characters. Notably, this adds in relevant information about where that chunk came from (based on the HTML) |  \n",
    "| Markdown | Markdown specific characters | ✅ | Splits text based on Markdown-specific characters. Notably, this adds in relevant information about where that chunk came from (based on the Markdown) |  \n",
    "| Code | Code (Python, JS) specific characters | . | Splits text based on characters specific to coding languages. 15 different languages are available to choose from. |\n",
    "| Token | Tokens | . | Splits text on tokens. There exist a few different ways to measure tokens. |\n",
    "| [Experimental] Semantic Chunker | Sentences | . | First splits on sentences. Then combines ones next to each other if they are semantically similar enough. Taken from Greg Kamradt |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ff841d-dcf6-4f19-b401-db61d939276a",
   "metadata": {},
   "source": [
    "## **Loading the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb23470b-1ba7-497c-aeb1-411128b8a73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kanavbansal/Developer/.env_jupyter/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"data/text/file_1.txt\")\n",
    "\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f915777e-1799-4963-9ea2-d7351a03333d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is pretty much\n",
      "what's happened so far.\n",
      "\n",
      "Ross was in love\n",
      "with Rachel since forever.\n",
      "\n",
      "Every time he tried to tell her,\n",
      "something got in the way\n",
      "\n",
      "Iike cats, Italian guys.\n",
      "\n",
      "And finally, Chandler was,\n",
      "like, \"Forget about her.\"\n"
     ]
    }
   ],
   "source": [
    "print(data[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33c6dd8a-6bde-447b-abff-44eb234adf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_content = data[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c7f3c6-7c0d-4103-a6ef-7af1c16a2515",
   "metadata": {},
   "source": [
    "## **1. Split by Character - CharacterTextSplitter**\n",
    "\n",
    "This is the simplest method. This splits based on characters (by default “”) and measure chunk length by number of characters.\n",
    "\n",
    "1. **How the text is split:** by single character.\n",
    "2. **How the chunk size is measured:** by number of characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf304c1a-9477-4eaf-ab4f-c5e5a9023339",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\\n\",\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a93fd449-ebb8-41c7-b12b-6e05bd38c999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of texts variable: <class 'list'>\n",
      "\n",
      "Type of each object inside the list: <class 'str'>\n",
      "\n",
      "Total number of documents inside texts list: 2\n",
      "\n",
      "* Content of first chunk: This is pretty much\n",
      "what's happened so far.\n",
      "\n",
      "Ross was in love\n",
      "with Rachel since forever.\n",
      "\n",
      "Every time he tried to tell her,\n",
      "something got in the way\n",
      "\n",
      "Iike cats, Italian guys.\n",
      "\n",
      "* Content of second chunk: Iike cats, Italian guys.\n",
      "\n",
      "And finally, Chandler was,\n",
      "like, \"Forget about her.\"\n"
     ]
    }
   ],
   "source": [
    "chunks = text_splitter.split_text(doc_content)\n",
    "\n",
    "print(\"Type of texts variable:\", type(chunks))\n",
    "print()\n",
    "print(\"Type of each object inside the list:\", type(chunks[0]))\n",
    "print()\n",
    "print(\"Total number of documents inside texts list:\", len(chunks))\n",
    "print()\n",
    "print(\"* Content of first chunk:\", chunks[0])\n",
    "print()\n",
    "print(\"* Content of second chunk:\", chunks[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3be3bfa4-cfaa-4ae2-b901-ae73b972d2cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173\n",
      "78\n"
     ]
    }
   ],
   "source": [
    "for chunk in chunks:\n",
    "    print(len(chunk))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b589f0c-8ef8-4419-9c7b-f21159812365",
   "metadata": {},
   "source": [
    "### **Passing Metadata with Chunks**\n",
    "\n",
    "If there are more than one document, you should add some metadata with each chunk to identify which document it belongs to. \n",
    "\n",
    "Here’s an example of passing metadata along with the documents, notice that it is split along with the documents.\n",
    "\n",
    "We will use **create_documents()** function to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bd1f114-b536-43ca-8966-53476ca7ce68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 2/2 [00:00<00:00, 2397.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of Data Variable:  <class 'list'>\n",
      "Number of Documents: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "loader = DirectoryLoader('data/text', glob=\"*.txt\", show_progress=True, loader_cls=TextLoader)\n",
    "\n",
    "data = loader.load()\n",
    "\n",
    "print(\"Type of Data Variable: \", type(data))\n",
    "\n",
    "print(\"Number of Documents:\", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82abe001-dac1-44a5-ac57-fa0239045135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of variable: <class 'list'>\n",
      "\n",
      "Type of each object inside the list: <class 'langchain_core.documents.base.Document'>\n",
      "\n",
      "Total number of documents inside chunks: 3\n",
      "\n",
      "Content of chunks: [Document(metadata={'source': 'data/text/file_1.txt'}, page_content=\"This is pretty much\\nwhat's happened so far.\\n\\nRoss was in love\\nwith Rachel since forever.\\n\\nEvery time he tried to tell her,\\nsomething got in the way\\n\\nIike cats, Italian guys.\"), Document(metadata={'source': 'data/text/file_1.txt'}, page_content='Iike cats, Italian guys.\\n\\nAnd finally, Chandler was,\\nlike, \"Forget about her.\"'), Document(metadata={'source': 'data/text/file_2.txt'}, page_content=\"These were unbelievely expensive\\nand he'll grow out of them\\n\\nin 20 minutes,\\nbut I couldn't resist!\\n\\nLook at these.\")]\n"
     ]
    }
   ],
   "source": [
    "# metadatas = [{\"document\": 1}, {\"document\": 2}]\n",
    "doc_contents = [doc.page_content for doc in data]\n",
    "meta_datas = [doc.metadata for doc in data]\n",
    "\n",
    "chunks = text_splitter.create_documents(doc_contents, meta_datas)\n",
    "\n",
    "print(\"Type of variable:\", type(chunks))\n",
    "print()\n",
    "print(\"Type of each object inside the list:\", type(chunks[0]))\n",
    "print()\n",
    "print(\"Total number of documents inside chunks:\", len(chunks))\n",
    "print()\n",
    "print(\"Content of chunks:\", chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976cd4f6-d44f-4272-9aba-9697b0880cd2",
   "metadata": {},
   "source": [
    "## **2. Recursively split by character**\n",
    "\n",
    "This text splitter is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is `[\"\\n\\n\", \"\\n\", \" \", \"\"]`. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text.\n",
    "\n",
    "1. **How the text is split:** by list of characters.\n",
    "2. **How the chunk size is measured:** by number of characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6ba0ce5-22e1-443f-a4b4-f2d8976a924f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of variable: <class 'list'>\n",
      "\n",
      "Type of each object inside the list: <class 'langchain_core.documents.base.Document'>\n",
      "\n",
      "Total number of documents inside list: 3\n",
      "\n",
      "* Content of first chunk: page_content='This is pretty much\n",
      "what's happened so far.\n",
      "\n",
      "Ross was in love\n",
      "with Rachel since forever.\n",
      "\n",
      "Every time he tried to tell her,\n",
      "something got in the way\n",
      "\n",
      "Iike cats, Italian guys.' metadata={'source': 'data/text/file_1.txt'}\n",
      "\n",
      "* Content of second chunk: page_content='Iike cats, Italian guys.\n",
      "\n",
      "And finally, Chandler was,\n",
      "like, \"Forget about her.\"' metadata={'source': 'data/text/file_1.txt'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=50,\n",
    ")\n",
    "\n",
    "chunks = text_splitter.create_documents(doc_contents, meta_datas)\n",
    "\n",
    "print(\"Type of variable:\", type(chunks))\n",
    "print()\n",
    "print(\"Type of each object inside the list:\", type(chunks[0]))\n",
    "print()\n",
    "print(\"Total number of documents inside list:\", len(chunks))\n",
    "print()\n",
    "print(\"* Content of first chunk:\", chunks[0])\n",
    "print()\n",
    "print(\"* Content of second chunk:\", chunks[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b792c64b-af6c-4b54-8885-e5ec3992b574",
   "metadata": {},
   "source": [
    "## **3. Split by tokens**\n",
    "\n",
    "Language models have a token limit. You should not exceed the token limit. When you split your text into chunks it is therefore a good idea to count the number of tokens. There are many tokenizers. When you count tokens in your text you should use the same tokenizer as used in the language model.\n",
    "\n",
    "1. nltk tokenizer\n",
    "```python\n",
    "from langchain_text_splitters import NLTKTextSplitter\n",
    "text_splitter = NLTKTextSplitter(chunk_size=1000)\n",
    "texts = text_splitter.split_text(text_string)\n",
    "```\n",
    "2. spaCy tokenizer\n",
    "```python\n",
    "from langchain_text_splitters import SpacyTextSplitter\n",
    "text_splitter = SpacyTextSplitter(chunk_size=1000)\n",
    "texts = text_splitter.split_text(text_string)\n",
    "```\n",
    "3. tiktoken (created by OpenAI)\n",
    "```python\n",
    "# tiktoken is a fast BPE tokenizer created by OpenAI.\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    encoding=\"cl100k_base\", chunk_size=100, chunk_overlap=0\n",
    ")\n",
    "texts = text_splitter.split_text(text_string)\n",
    "```\n",
    "4. sentence-transformers tokenizer\n",
    "\n",
    "5. KoNLPY (for Korean Language)\n",
    "\n",
    "6. Hugging Face tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecee2fdc-5063-4cc9-b60b-cf9cb00250b7",
   "metadata": {},
   "source": [
    "### **NLTK Text Splitter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81e991a8-93df-4a7f-aea7-28fb3a377671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "528359a5-05dc-4d5d-8a38-af86eae0b69c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/kanavbansal/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0744aa3-f065-458d-85ec-62197f83de26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of variable: <class 'list'>\n",
      "\n",
      "Type of each object inside the list: <class 'langchain_core.documents.base.Document'>\n",
      "\n",
      "Total number of documents inside list: 5\n",
      "\n",
      "* Content of first chunk: page_content='This is pretty much\n",
      "what's happened so far.\n",
      "\n",
      "Ross was in love\n",
      "with Rachel since forever.' metadata={'source': 'data/text/file_1.txt'}\n",
      "\n",
      "* Content of second chunk: page_content='Every time he tried to tell her,\n",
      "something got in the way\n",
      "\n",
      "Iike cats, Italian guys.' metadata={'source': 'data/text/file_1.txt'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import NLTKTextSplitter\n",
    "\n",
    "nltk_text_splitter = NLTKTextSplitter(chunk_size=100, chunk_overlap=50)\n",
    "\n",
    "chunks = nltk_text_splitter.create_documents(doc_contents, meta_datas)\n",
    "\n",
    "print(\"Type of variable:\", type(chunks))\n",
    "print()\n",
    "print(\"Type of each object inside the list:\", type(chunks[0]))\n",
    "print()\n",
    "print(\"Total number of documents inside list:\", len(chunks))\n",
    "print()\n",
    "print(\"* Content of first chunk:\", chunks[0])\n",
    "print()\n",
    "print(\"* Content of second chunk:\", chunks[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3f74783-5828-410b-a432-236ce538a6a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'data/text/file_1.txt'}, page_content=\"This is pretty much\\nwhat's happened so far.\\n\\nRoss was in love\\nwith Rachel since forever.\"), Document(metadata={'source': 'data/text/file_1.txt'}, page_content='Every time he tried to tell her,\\nsomething got in the way\\n\\nIike cats, Italian guys.'), Document(metadata={'source': 'data/text/file_1.txt'}, page_content='And finally, Chandler was,\\nlike, \"Forget about her.\"'), Document(metadata={'source': 'data/text/file_2.txt'}, page_content=\"These were unbelievely expensive\\nand he'll grow out of them\\n\\nin 20 minutes,\\nbut I couldn't resist!\"), Document(metadata={'source': 'data/text/file_2.txt'}, page_content='Look at these.')]\n"
     ]
    }
   ],
   "source": [
    "print(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3da0d3-0f93-4371-9c7a-b249c2c18d49",
   "metadata": {},
   "source": [
    "### **tiktoken Text Splitter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9fcd015-ce15-4d81-a1d6-f648b672f219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3167ee4-bcc6-4e39-9577-ceb6b3ea271d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of variable: <class 'list'>\n",
      "\n",
      "Type of each object inside the list: <class 'langchain_core.documents.base.Document'>\n",
      "\n",
      "Total number of documents inside list: 3\n",
      "\n",
      "* Content of first chunk: page_content='This is pretty much\n",
      "what's happened so far.\n",
      "\n",
      "Ross was in love\n",
      "with Rachel since forever.\n",
      "\n",
      "Every time he tried to tell her,\n",
      "something got in the way\n",
      "\n",
      "Iike cats, Italian guys.' metadata={'source': 'data/text/file_1.txt'}\n",
      "\n",
      "* Content of second chunk: page_content='Iike cats, Italian guys.\n",
      "\n",
      "And finally, Chandler was,\n",
      "like, \"Forget about her.\"' metadata={'source': 'data/text/file_1.txt'}\n"
     ]
    }
   ],
   "source": [
    "# tiktoken is a fast BPE tokenizer created by OpenAI\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "                                                            model_name=\"gpt-3.5-turbo\",\n",
    "                                                            chunk_size=50, \n",
    "                                                            chunk_overlap=20\n",
    "                )\n",
    "\n",
    "chunks = text_splitter.create_documents(doc_contents, meta_datas)\n",
    "\n",
    "print(\"Type of variable:\", type(chunks))\n",
    "print()\n",
    "print(\"Type of each object inside the list:\", type(chunks[0]))\n",
    "print()\n",
    "print(\"Total number of documents inside list:\", len(chunks))\n",
    "print()\n",
    "print(\"* Content of first chunk:\", chunks[0])\n",
    "print()\n",
    "print(\"* Content of second chunk:\", chunks[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0945e4-27a9-4f85-a369-7106c981ff50",
   "metadata": {},
   "source": [
    "### **Sentence Transformers Token Text Splitter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "790aa383-9c3b-4de4-ac3a-6054a5fe8454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54c5423d-42e1-4935-a982-a34c56c1de26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kanavbansal/Developer/.env_jupyter/lib/python3.9/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of variable: <class 'list'>\n",
      "\n",
      "Type of each object inside the list: <class 'langchain_core.documents.base.Document'>\n",
      "\n",
      "Total number of documents inside list: 2\n",
      "\n",
      "* Content of first chunk: page_content='this is pretty much what's happened so far. ross was in love with rachel since forever. every time he tried to tell her, something got in the way iike cats, italian guys. and finally, chandler was, like, \" forget about her. \"' metadata={'source': 'data/text/file_1.txt'}\n",
      "\n",
      "* Content of second chunk: page_content='these were unbelievely expensive and he'll grow out of them in 20 minutes, but i couldn't resist! look at these.' metadata={'source': 'data/text/file_2.txt'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters.sentence_transformers import SentenceTransformersTokenTextSplitter\n",
    "\n",
    "st_text_splitter = SentenceTransformersTokenTextSplitter(model_name=\"sentence-transformers/all-mpnet-base-v2\", \n",
    "                                                         chunk_size=100, \n",
    "                                                         chunk_overlap=50)\n",
    "\n",
    "chunks = st_text_splitter.create_documents(doc_contents, meta_datas)\n",
    "\n",
    "print(\"Type of variable:\", type(chunks))\n",
    "print()\n",
    "print(\"Type of each object inside the list:\", type(chunks[0]))\n",
    "print()\n",
    "print(\"Total number of documents inside list:\", len(chunks))\n",
    "print()\n",
    "print(\"* Content of first chunk:\", chunks[0])\n",
    "print()\n",
    "print(\"* Content of second chunk:\", chunks[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
