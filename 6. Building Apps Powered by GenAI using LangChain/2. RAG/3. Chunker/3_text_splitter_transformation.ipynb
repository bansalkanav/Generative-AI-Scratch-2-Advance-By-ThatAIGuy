{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b355560-7854-4da4-8340-74e406194393",
   "metadata": {},
   "source": [
    "# **Text Transformation using Text Splitters**\n",
    "\n",
    "Notice that after you load a Documnet object from a source, you end up with strings by grabbing them from page_content. In certain situations, the length of the strings may be too large to feed into a model, both embedding and chat model.\n",
    "\n",
    "**Note: LLMs have fixed maximum context window. If you document contains more token then the maximum context window size, LLMs won't be able to process it. So, it is important to break the documents into chunks.**\n",
    "\n",
    "Once you've loaded documents, you'll often want to transform them to better suit your application. The simplest example is you may want to split a long document into smaller chunks that can fit into your model's context window. LangChain has a number of built-in document transformers that make it easy to split, combine, filter, and otherwise manipulate documents.\n",
    "\n",
    "When you want to deal with long pieces of text, it is necessary to split up that text into chunks. As simple as this sounds, there is a lot of potential complexity here. Ideally, you want to keep the semantically related pieces of text together. What \"semantically related\" means could depend on the type of text. This notebook showcases several ways to do that.\n",
    "\n",
    "At a high level, text splitters work as following:\n",
    "\n",
    "1. Split the text up into small, semantically meaningful chunks (often sentences).\n",
    "2. Start combining these small chunks into a larger chunk until you reach a certain size (as measured by some function).\n",
    "3. Once you reach that size, make that chunk its own piece of text and then start creating a new chunk of text with some overlap (to keep context between chunks).\n",
    "\n",
    "That means there are two different axes along which you can customize your text splitter:\n",
    "\n",
    "1. How the text is split\n",
    "2. How the chunk size is measured\n",
    "\n",
    "## **Types of Text Splitters**\n",
    "\n",
    "**Evaluate text splitters**  \n",
    "You can evaluate text splitters with the [Chunkviz utility](https://www.chunkviz.com/) created by Greg Kamradt. Chunkviz is a great tool for visualizing how your text splitter is working. It will show you how your text is being split up and help in tuning up the splitting parameters.\n",
    "\n",
    "LangChain offers many different types of text splitters. These all live in the `langchain-text-splitters` package. Below is a table listing all of them, along with a few characteristics:\n",
    "> **Name:** Name of the text splitter  \n",
    "> **Splits On:** How this text splitter splits text  \n",
    "> **Adds Metadata:** Whether or not this text splitter adds metadata about where each chunk came from.  \n",
    "> **Description:** Description of the splitter, including recommendation on when to use it.\n",
    "\n",
    "| Name | Splits On | Adds Metadata | Description |\n",
    "| :--- | :--- | :---: | :--- |\n",
    "| Recursive | A list of user defined characters | . | Recursively splits text. Splitting text recursively serves the purpose of trying to keep related pieces of text next to each other. This is the recommended way to start splitting text. |\n",
    "| HTML | HTML specific characters | ✅ | Splits text based on HTML-specific characters. Notably, this adds in relevant information about where that chunk came from (based on the HTML) |\n",
    "| Markdown | Markdown specific characters | ✅ | Splits text based on Markdown-specific characters. Notably, this adds in relevant information about where that chunk came from (based on the Markdown) |\n",
    "| Code | Code (Python, JS) specific characters | . | Splits text based on characters specific to coding languages. 15 different languages are available to choose from.\n",
    "| Token | Tokens | . | Splits text on tokens. There exist a few different ways to measure tokens. |\n",
    "| Character | A user defined character | . | Splits text based on a user defined character. One of the simpler methods. |\n",
    "| [Experimental] Semantic Chunker | Sentences | . | First splits on sentences. Then combines ones next to each other if they are semantically similar enough. Taken from Greg Kamradt |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ff841d-dcf6-4f19-b401-db61d939276a",
   "metadata": {},
   "source": [
    "## **Loading the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb23470b-1ba7-497c-aeb1-411128b8a73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"data/subtitles_data/Friends - [2x01] - The One with Ross's New Girlfriend.srt\")\n",
    "\n",
    "doc = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f915777e-1799-4963-9ea2-d7351a03333d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"1\\n00:00:01,435 --> 00:00:04,082\\nThis is pretty much\\nwhat's happened so far.\\n\\n2\\n00:00:04,395 --> 00:0\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[0].page_content[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33c6dd8a-6bde-447b-abff-44eb234adf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_content = doc[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c7f3c6-7c0d-4103-a6ef-7af1c16a2515",
   "metadata": {},
   "source": [
    "## **1. Split by Character**\n",
    "\n",
    "This is the simplest method. This splits based on characters (by default “”) and measure chunk length by number of characters.\n",
    "\n",
    "1. **How the text is split:** by single character.\n",
    "2. **How the chunk size is measured:** by number of characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf304c1a-9477-4eaf-ab4f-c5e5a9023339",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\\n\",\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a93fd449-ebb8-41c7-b12b-6e05bd38c999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of texts variable: <class 'list'>\n",
      "Type of each object inside the list: <class 'langchain_core.documents.base.Document'>\n",
      "Total number of documents inside texts list: 140\n",
      "Content of first document: page_content=\"1\\n00:00:01,435 --> 00:00:04,082\\nThis is pretty much\\nwhat's happened so far.\\n\\n2\\n00:00:04,395 --> 00:00:07,179\\nRoss was in love\\nwith Rachel since forever.\"\n"
     ]
    }
   ],
   "source": [
    "texts = text_splitter.create_documents([doc_content])\n",
    "\n",
    "print(\"Type of texts variable:\", type(texts))\n",
    "\n",
    "print(\"Type of each object inside the list:\", type(texts[0]))\n",
    "\n",
    "print(\"Total number of documents inside texts list:\", len(texts))\n",
    "\n",
    "print(\"Content of first document:\", texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3be3bfa4-cfaa-4ae2-b901-ae73b972d2cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"1\\n00:00:01,435 --> 00:00:04,082\\nThis is pretty much\\nwhat's happened so far.\\n\\n2\\n00:00:04,395 --> 00:00:07,179\\nRoss was in love\\nwith Rachel since forever.\"),\n",
       " Document(page_content='3\\n00:00:07,423 --> 00:00:10,437\\nEvery time he tried to tell her,\\nsomething got in the way...\\n\\n4\\n00:00:10,651 --> 00:00:12,529\\n...Iike cats, Italian guys.'),\n",
       " Document(page_content='5\\n00:00:12,736 --> 00:00:15,922\\nAnd finally, Chandler was,\\nlike, \"Forget about her.\"\\n\\n6\\n00:00:16,166 --> 00:00:20,762\\nWhen Ross was in China, Chandler\\nlet it slip that Ross loved Rachel.'),\n",
       " Document(page_content='7\\n00:00:20,975 --> 00:00:22,818\\nShe was, like, \"Oh, my God!\"\\n\\n8\\n00:00:23,061 --> 00:00:25,845\\nSo she went to the airport to meet him.'),\n",
       " Document(page_content=\"9\\n00:00:26,089 --> 00:00:29,710\\nShe didn 't know Ross was getting\\noff the plane with another woman.\\n\\n10\\n00:00:31,165 --> 00:00:33,651\\nThat's pretty much everything\\nyou need to know.\")]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b589f0c-8ef8-4419-9c7b-f21159812365",
   "metadata": {},
   "source": [
    "### **Passing Metadata with Documents**\n",
    "\n",
    "If there are more than one document, you should add some metadata with each chunk to identify which document it belongs to. \n",
    "\n",
    "Here’s an example of passing metadata along with the documents, notice that it is split along with the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82abe001-dac1-44a5-ac57-fa0239045135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of texts variable: <class 'list'>\n",
      "Type of each object inside the list: <class 'langchain_core.documents.base.Document'>\n",
      "Total number of documents inside texts list: 280\n",
      "Content of first document: page_content=\"1\\n00:00:01,435 --> 00:00:04,082\\nThis is pretty much\\nwhat's happened so far.\\n\\n2\\n00:00:04,395 --> 00:00:07,179\\nRoss was in love\\nwith Rachel since forever.\" metadata={'document': 1}\n"
     ]
    }
   ],
   "source": [
    "metadatas = [{\"document\": 1}, {\"document\": 2}]\n",
    "\n",
    "chunks = text_splitter.create_documents(\n",
    "    [doc_content, doc_content], metadatas=metadatas\n",
    ")\n",
    "\n",
    "print(\"Type of texts variable:\", type(chunks))\n",
    "\n",
    "print(\"Type of each object inside the list:\", type(chunks[0]))\n",
    "\n",
    "print(\"Total number of documents inside texts list:\", len(chunks))\n",
    "\n",
    "print(\"Content of first document:\", chunks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2f28865-9b96-44fc-be8a-de17398188a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content=\"1\\n00:00:01,435 --> 00:00:04,082\\nThis is pretty much\\nwhat's happened so far.\\n\\n2\\n00:00:04,395 --> 00:00:07,179\\nRoss was in love\\nwith Rachel since forever.\" metadata={'document': 1}\n",
      "\n",
      "page_content=\"351\\n00:23:48,261 --> 00:23:51,360\\nAndie MacDowell is the guy\\nfrom Planet of the Apes.\\n\\n352\\n00:23:54,450 --> 00:23:56,316\\n-Thank you.\\n-You're welcome.\" metadata={'document': 2}\n"
     ]
    }
   ],
   "source": [
    "print(chunks[0])\n",
    "print()\n",
    "print(chunks[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976cd4f6-d44f-4272-9aba-9697b0880cd2",
   "metadata": {},
   "source": [
    "## **2. Recursively split by character**\n",
    "\n",
    "This text splitter is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is `[\"\\n\\n\", \"\\n\", \" \", \"\"]`. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text.\n",
    "\n",
    "1. **How the text is split:** by list of characters.\n",
    "2. **How the chunk size is measured:** by number of characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6ba0ce5-22e1-443f-a4b4-f2d8976a924f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of texts variable: <class 'list'>\n",
      "Type of each object inside the list: <class 'langchain_core.documents.base.Document'>\n",
      "Total number of documents inside texts list: 335\n",
      "Content of first document: page_content=\"1\\n00:00:01,435 --> 00:00:04,082\\nThis is pretty much\\nwhat's happened so far.\"\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "texts = text_splitter.create_documents([doc_content])\n",
    "\n",
    "print(\"Type of texts variable:\", type(texts))\n",
    "\n",
    "print(\"Type of each object inside the list:\", type(texts[0]))\n",
    "\n",
    "print(\"Total number of documents inside texts list:\", len(texts))\n",
    "\n",
    "print(\"Content of first document:\", texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d92cdebc-ced0-4728-9532-78c4ea8816f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content=\"1\\n00:00:01,435 --> 00:00:04,082\\nThis is pretty much\\nwhat's happened so far.\"\n",
      "\n",
      "page_content='2\\n00:00:04,395 --> 00:00:07,179\\nRoss was in love\\nwith Rachel since forever.'\n"
     ]
    }
   ],
   "source": [
    "print(texts[0])\n",
    "print()\n",
    "print(texts[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b792c64b-af6c-4b54-8885-e5ec3992b574",
   "metadata": {},
   "source": [
    "## **Split by tokens**\n",
    "\n",
    "Language models have a token limit. You should not exceed the token limit. When you split your text into chunks it is therefore a good idea to count the number of tokens. There are many tokenizers. When you count tokens in your text you should use the same tokenizer as used in the language model.\n",
    "\n",
    "1. nltk tokenizer\n",
    "```python\n",
    "from langchain_text_splitters import NLTKTextSplitter\n",
    "text_splitter = NLTKTextSplitter(chunk_size=1000)\n",
    "texts = text_splitter.split_text(text_string)\n",
    "```\n",
    "2. spaCy tokenizer\n",
    "```python\n",
    "from langchain_text_splitters import SpacyTextSplitter\n",
    "text_splitter = SpacyTextSplitter(chunk_size=1000)\n",
    "texts = text_splitter.split_text(text_string)\n",
    "```\n",
    "3. tiktoken (created by OpenAI)\n",
    "```python\n",
    "# tiktoken is a fast BPE tokenizer created by OpenAI.\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    encoding=\"cl100k_base\", chunk_size=100, chunk_overlap=0\n",
    ")\n",
    "texts = text_splitter.split_text(text_string)\n",
    "```\n",
    "4. sentence-transformers tokenizer\n",
    "\n",
    "5. KoNLPY (for Korean Language)\n",
    "\n",
    "6. Hugging Face tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecee2fdc-5063-4cc9-b60b-cf9cb00250b7",
   "metadata": {},
   "source": [
    "### **NLTK Text Splitter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0744aa3-f065-458d-85ec-62197f83de26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import NLTKTextSplitter\n",
    "\n",
    "nltk_text_splitter = NLTKTextSplitter(chunk_size=1000)\n",
    "\n",
    "texts = nltk_text_splitter.split_text(doc_content)\n",
    "\n",
    "print(len(texts))\n",
    "\n",
    "print(type(texts[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3f74783-5828-410b-a432-236ce538a6a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "00:00:01,435 --> 00:00:04,082\n",
      "This is pretty much\n",
      "what's happened so far.\n",
      "\n",
      "2\n",
      "00:00:04,395 --> 00:00:07,179\n",
      "Ross was in love\n",
      "with Rachel since forever.\n",
      "\n",
      "3\n",
      "00:00:07,423 --> 00:00:10,437\n",
      "Every time he tried to tell her,\n",
      "something got in the way...\n",
      "\n",
      "4\n",
      "00:00:10,651 --> 00:00:12,529\n",
      "...Iike cats, Italia\n"
     ]
    }
   ],
   "source": [
    "print(texts[0][:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499a55ae-e846-4431-b137-fed4cb0d9bc0",
   "metadata": {},
   "source": [
    "### **Spacy Text Splitter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "278b7782-ea9d-4341-a024-81c3e0fce62c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "<class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\spacy\\pipeline\\lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import SpacyTextSplitter\n",
    "\n",
    "spacy_text_splitter = SpacyTextSplitter(chunk_size=1000)\n",
    "\n",
    "texts = spacy_text_splitter.split_text(doc_content)\n",
    "\n",
    "print(len(texts))\n",
    "\n",
    "print(type(texts[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49f1548e-8df0-4f58-9c4b-27b9ca187861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "00:00:01,435 --> 00:00:04,082\n",
      "\n",
      "\n",
      "This is pretty much\n",
      "what's happened so far.\n",
      "\n",
      "\n",
      "\n",
      "2\n",
      "00:00:04,395 --> 00:00:07,179\n",
      "Ross was in love\n",
      "with Rachel since forever.\n",
      "\n",
      "\n",
      "\n",
      "3\n",
      "00:00:07,423 --> 00:00:10,437\n",
      "\n",
      "\n",
      "Every time he tried to tell her,\n",
      "something got in the way...\n",
      "\n",
      "4\n",
      "00:00:10,651 --> 00:00:12,529\n",
      "...\n",
      "\n",
      "Iike ca\n"
     ]
    }
   ],
   "source": [
    "print(texts[0][:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3da0d3-0f93-4371-9c7a-b249c2c18d49",
   "metadata": {},
   "source": [
    "### **tiktoken Text Splitter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9fcd015-ce15-4d81-a1d6-f648b672f219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3167ee4-bcc6-4e39-9577-ceb6b3ea271d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# tiktoken is a fast BPE tokenizer created by OpenAI.\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size=500)\n",
    "\n",
    "texts = text_splitter.split_text(doc_content)\n",
    "\n",
    "print(len(texts))\n",
    "\n",
    "print(type(texts[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "268510b2-2a29-48f1-b3c3-8be8ade9b7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "00:00:01,435 --> 00:00:04,082\n",
      "This is pretty much\n",
      "what's happened so far.\n",
      "\n",
      "2\n",
      "00:00:04,395 --> 00:00:07,179\n",
      "Ross was in love\n",
      "with Rachel since forever.\n",
      "\n",
      "3\n",
      "00:00:07,423 --> 00:00:10,437\n",
      "Every time he tried to tell her,\n",
      "something got in the way...\n",
      "\n",
      "4\n",
      "00:00:10,651 --> 00:00:12,529\n",
      "...Iike cats, Italia\n"
     ]
    }
   ],
   "source": [
    "print(texts[0][:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76860777-5fb0-44bd-ab0c-be8f32777cf4",
   "metadata": {},
   "source": [
    "## **Semantic Chunking**\n",
    "\n",
    "Splits the text based on semantic similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8784e6dc-f0fd-47c4-b282-d23ea3a62fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = open('keys/.openai_api_key.txt')\n",
    "# OPENAI_API_KEY = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6315ecb7-8792-468b-93fe-424b3e5591ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# embedding_model = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fff8ff6c-bb17-4201-afd6-ae7186eae006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_experimental.text_splitter import SemanticChunker\n",
    "\n",
    "# text_splitter = SemanticChunker(embedding_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
