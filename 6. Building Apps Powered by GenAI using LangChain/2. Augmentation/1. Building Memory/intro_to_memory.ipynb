{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f5c86a2-76b6-4cf7-8f94-051684e7784d",
   "metadata": {},
   "source": [
    "# **Memory**\n",
    "\n",
    "Memory is a cognitive function that allows people to store, retrieve, and use information to understand their present and future. Consider the frustration of working with a colleague who forgets everything you tell them, requiring constant repetition! As AI agents undertake more complex tasks involving numerous user interactions, equipping them with memory becomes equally crucial for efficiency and user satisfaction. With memory, agents can learn from feedback and adapt to users' preferences. \n",
    "\n",
    "Most LLM applications have a conversational interface. An essential component of a conversation is being able to refer to information introduced earlier in the conversation. At bare minimum, a conversational system should be able to access some window of past messages directly. A more complex system will need to have a world model that it is constantly updating, which allows it to do things like maintain information about entities and their relationships.\n",
    "\n",
    "We call this ability to store information about past interactions \"memory\". \n",
    "\n",
    "## **Building memory into a system**\n",
    "The two core design decisions in any memory system are:\n",
    "- How state is stored\n",
    "- How state is queried\n",
    "\n",
    "## **Depricated**\n",
    "- ConversationBufferMemory\n",
    "- ConversationStringBufferMemory\n",
    "- ConversationBufferWindowMemory\n",
    "- ConversationTokenBufferMemory\n",
    "- ConversationSummaryMemory\n",
    "- ConversationSummaryBufferMemory\n",
    "- VectorStoreRetrieverMemory\n",
    "\n",
    "## **What's covered?**\n",
    "- Designing Memory - Buidling End-to-end Conversational AI Bot\n",
    "- Saving and Loading a Chat History\n",
    "- SQLChatMessageHistory and Adding Session ID\n",
    "- Introduction to LangGraph for Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1210d13f-efab-42cd-bfa9-3d04865048bb",
   "metadata": {},
   "source": [
    "## **Designing Memory - Buidling End-to-end Conversational AI Bot**\n",
    "\n",
    "<img src=\"images/memory.png\">\n",
    "\n",
    "### **Steps:**\n",
    "1. Import Chat Model and Configure the API Key\n",
    "2. Create Chat Template\n",
    "3. Create a Output Parser\n",
    "4. Initialize the Memory\n",
    "5. Build a Chain\n",
    "6. Invoke the chain with human_input and chat_history\n",
    "7. Saving to memory\n",
    "8. Run Step 6 and 7 in a loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c109071-82b9-4628-a1eb-a0fcfe7588bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kanavbansal/Developer/.env_jupyter/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Step 1 - Import Chat Model and Configure the API Key\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Setup API Key\n",
    "f = open('keys/.openai_api_key.txt')\n",
    "OPENAI_API_KEY = f.read()\n",
    "\n",
    "# Set the OpenAI Key and initialize a ChatModel\n",
    "chat_model = ChatOpenAI(openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d991c8dd-ddb9-47b0-b483-ede46666e524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 - Create Chat Template\n",
    "\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        # The persistent system prompt\n",
    "        SystemMessage(\n",
    "            content=\"You are a chatbot having a conversation with a human.\"\n",
    "        ),\n",
    "        # Creating a chat_history placeholder\n",
    "        MessagesPlaceholder(\n",
    "            variable_name=\"chat_history\"\n",
    "        ),  \n",
    "        # Human Prompt\n",
    "        HumanMessagePromptTemplate.from_template(\n",
    "            \"{human_input}\"\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c5795c6-04cc-42dd-a874-29a2ab0fc2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 - Create a Output Parser\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70898aed-44e4-49cf-bd30-bf6832f4abbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4 - Initialize the Memory\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "memory_buffer = {\"history\": []}\n",
    "\n",
    "def get_history_from_buffer(human_input):\n",
    "    return memory_buffer[\"history\"]\n",
    "\n",
    "runnable_get_history_from_buffer = RunnableLambda(get_history_from_buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150d6c9f-caff-42db-8cda-1d51864ad344",
   "metadata": {},
   "source": [
    "#### **RunnablePassthrough:** RunnablePassthrough on its own allows you to pass inputs unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d6db02f-49ab-4ae3-af8d-810604ed99bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5 - Build a Chain (Another way)\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Define a chain\n",
    "chain = RunnablePassthrough.assign(\n",
    "        chat_history=runnable_get_history_from_buffer\n",
    "        ) | chat_template | chat_model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2dce5bc2-84d8-4c46-b8ac-48a2e82040f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello! I'm just a computer program so I don't have feelings, but I'm here and ready to help you with anything you need. How can I assist you today?\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 6 - Invoke the chain with human_input and chat_history\n",
    "\n",
    "query = {\"human_input\": \"Hi, How are you?\"}\n",
    "\n",
    "response = chain.invoke(query)\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "189a3dbc-59a7-4c4b-8038-2ddbc4ac5353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': []}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31f4b47a-9190-4f3d-82b6-d1b0f585a7b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi, How are you?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Hello! I'm just a computer program so I don't have feelings, but I'm here and ready to help you with anything you need. How can I assist you today?\", additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 7 - Saving to memory\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "memory_buffer[\"history\"].append(HumanMessage(content=query[\"human_input\"]))\n",
    "memory_buffer[\"history\"].append(AIMessage(content=response))\n",
    "\n",
    "memory_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de61677b-9f1f-4e23-a100-eb279342f5d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your input:  My name is Kanav\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*User: My name is Kanav\n",
      "*AI: Hello Kanav! It's nice to meet you. How can I assist you today?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your input:  just exploring\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*User: just exploring\n",
      "*AI: That's great! Feel free to ask me anything or share any topics you'd like to explore. I'm here to help and provide information.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your input:  that's good to know\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*User: that's good to know\n",
      "*AI: I'm glad to hear that! If you have any questions or need assistance, don't hesitate to ask. I'm here to help.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your input:  what;s my name?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*User: what;s my name?\n",
      "*AI: Your name is Kanav.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your input:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*User: exit\n"
     ]
    }
   ],
   "source": [
    "# Step 8 - Run Step 6 and 7 in a loop\n",
    "\n",
    "while True:\n",
    "    query = {\"human_input\" : input('Enter your input: ')}\n",
    "    print(f\"*User: {query['human_input']}\")\n",
    "    if query[\"human_input\"] in ['bye', 'quit', 'exit']:\n",
    "        break\n",
    "    response = chain.invoke(query)\n",
    "    print(f\"*AI: {response}\")\n",
    "\n",
    "    memory_buffer[\"history\"].append(HumanMessage(content=query[\"human_input\"]))\n",
    "    memory_buffer[\"history\"].append(AIMessage(content=response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc450acb-d899-4180-9a39-e480de710128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Hi, How are you?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to assist you. How can I help you today?\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='My name is Kanav', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"Hello Kanav! It's nice to meet you. How can I assist you today?\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='just exploring', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"That's great! Feel free to ask me anything or share any topics you'd like to explore. I'm here to help and provide information.\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content=\"that's good to know\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"I'm glad to hear that! If you have any questions or need assistance, don't hesitate to ask. I'm here to help.\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='what;s my name?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Your name is Kanav.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory_buffer[\"history\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46fd022-9da2-4d0b-9581-cf8363f36097",
   "metadata": {},
   "source": [
    "## **Saving a Chat History**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866f57c5-b8f5-4efc-a409-bf4e221629d4",
   "metadata": {},
   "source": [
    "**Let's now learn to save this history on the disk so that whenever we can load the history whenever we chat with our assistant.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8217df9c-fcd3-4f4d-baa2-85e413e7e9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "chat_history = pickle.dumps(memory_buffer)\n",
    "\n",
    "with open(\"chats_data/conversation_memory.pkl\", \"wb\") as f:\n",
    "    f.write(chat_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c5c8b1-4521-43ce-83f2-f1c84d3609d0",
   "metadata": {},
   "source": [
    "## **Loading a Chat History**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9ca65b7-c292-45f2-92e4-20f84679277a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history_loaded = pickle.load(open(\"chats_data/conversation_memory.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35fb23db-84d4-45cf-82a9-1e46184fba98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi, How are you?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to assist you. How can I help you today?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='My name is Kanav', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Hello Kanav! It's nice to meet you. How can I assist you today?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='just exploring', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's great! Feel free to ask me anything or share any topics you'd like to explore. I'm here to help and provide information.\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"that's good to know\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"I'm glad to hear that! If you have any questions or need assistance, don't hesitate to ask. I'm here to help.\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='what;s my name?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Your name is Kanav.', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history_loaded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a57196c-f3b8-49d3-b812-8c4d999e6adf",
   "metadata": {},
   "source": [
    "## **SQLChatMessageHistory**\n",
    "\n",
    "`ChatMessageHistory` allows us to store separate conversation histories per user or session which is often done by the real-time chatbots. `session_id` is used to distinguish between separate conversations.\n",
    "\n",
    "In order to use it, we can use a `get_session_history` function which take `session_id` and returns a message history object.\n",
    "\n",
    "There is a support of many `Memory` components under `langchain_community.chat_message_histories`, like:\n",
    "1. AstraDBChatMessageHistory\n",
    "2. DynamoDBChatMessageHistory\n",
    "3. CassandraChatMessageHistory\n",
    "4. ElasticsearchChatMessageHistory\n",
    "5. KafkaChatMessageHistory\n",
    "6. MongoDBChatMessageHistory\n",
    "7. RedisChatMessageHistory\n",
    "8. PostgresChatMessageHistory\n",
    "9. SQLChatMessageHistory\n",
    "\n",
    "**[Click Here](https://python.langchain.com/v0.2/docs/integrations/memory/)** to read more.\n",
    "\n",
    "### **Usage**\n",
    "\n",
    "To use the storage you need to provide only 2 things:\n",
    "\n",
    "1. **Session Id** - a unique identifier of the session, like user name, email, chat id etc.\n",
    "2. **Connection string**\n",
    "    - For SQL (SQLAlchemy) - A string that specifies the database connection. It will be passed to SQLAlchemy create_engine function.\n",
    "    - For SQLite - A string that specifies the database connection. For SQLite, that string is slqlite:/// followed by the name of the database file. If that file doesn't exist, it will be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "def066d9-117f-481a-942a-69d0dd8d483e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import SQLChatMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8de1ed9-082b-40ce-bb96-69dc61022146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the OpenAI Key and initialize a ChatModel\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat_model = ChatOpenAI(openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "73fcc4a9-2e7a-4f83-bc25-4c95d573ec74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a connection with the database and \n",
    "# return the chat message history for a session id\n",
    "\n",
    "from langchain_community.chat_message_histories import SQLChatMessageHistory\n",
    "\n",
    "def get_session_message_history_from_db(session_id):\n",
    "    chat_message_history = SQLChatMessageHistory(\n",
    "                                   session_id=session_id, \n",
    "                                   connection=\"sqlite:///chats_data/sqlite.db\"\n",
    "                               )\n",
    "    return chat_message_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b5043c4-3fa1-484f-beae-937766b8741e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a chat template\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "chat_template = ChatPromptTemplate(\n",
    "                messages=[\n",
    "                    (\"system\", \"You are a helpful AI assistant.\"), \n",
    "                    MessagesPlaceholder(variable_name=\"history\"), \n",
    "                    (\"human\", \"{human_input}\")\n",
    "                ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "af3bac9b-9370-4a60-8fad-23b175e1065c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the chain\n",
    "\n",
    "chain = chat_template | chat_model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "989e9909-67fc-4afa-a369-ef0e231b856e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use RunnableWithMessageHistory to load \n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "conversation_chain = RunnableWithMessageHistory(\n",
    "                        chain, \n",
    "                        get_session_message_history_from_db,\n",
    "                        input_messages_key=\"human_input\", \n",
    "                        history_messages_key=\"history\"\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e00998b-2fe4-4ada-a48c-7da04c4da47b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes, ThatAIGuy, the capital of Himachal Pradesh is Shimla.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is where we configure the session id\n",
    "user_id = \"thataiguy\"\n",
    "config = {\"configurable\": {\"session_id\": user_id}}\n",
    "\n",
    "input_prompt = {\"human_input\": \"My name is ThatAIGuy. Can you tell me the capital of Himachal?\"}\n",
    "response = conversation_chain.invoke(input_prompt, config=config)\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "361e0eaa-bd4d-4617-94be-ba5a2e0311a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The biggest state in India by area is Rajasthan.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is where we configure the session id\n",
    "user_id = \"kanav\"\n",
    "config = {\"configurable\": {\"session_id\": user_id}}\n",
    "\n",
    "input_prompt = {\"human_input\": \"My name is Kanav Bansal. What is the biggest state in India?\"}\n",
    "response = conversation_chain.invoke(input_prompt, config=config)\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "815f32f8-c0ec-4cd8-a52a-80e9adda6daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_bot(session_id, prompt):\n",
    "    config = {\"configurable\": {\"session_id\": user_id}}\n",
    "    input_prompt = {\"human_input\": prompt}\n",
    "\n",
    "    response = conversation_chain.invoke(input_prompt, config=config)\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "78c27159-25a8-43bd-ad05-6a8bba6f91a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes, your name is ThatAIGuy.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_id = \"thataiguy\"\n",
    "input_prompt = \"Do you remember my name?\"\n",
    "chat_bot(session_id=user_id, prompt=input_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a8122a47-96a9-4afd-a328-7e28909594b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes, your name is Kanav Bansal.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_id = \"kanav\"\n",
    "input_prompt = \"Do you remember my name?\"\n",
    "chat_bot(session_id=user_id, prompt=input_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e619479-d835-4290-ac7f-f0077badd475",
   "metadata": {},
   "source": [
    "## **Introduction to LangGraph for Memory**\n",
    "\n",
    "As of the v0.3 release, LangChain recommends that users take advantage of LangGraph persistence to incorporate memory into new LangChain applications.\n",
    "\n",
    "If your code is already relying on `RunnableWithMessageHistory` or `BaseChatMessageHistory`, you do not need to make any changes. We do not plan on deprecating this functionality in the near future as it works for simple chat applications and any code that uses `RunnableWithMessageHistory` will continue to work as expected.\n",
    "\n",
    "In this guide we cover two types of memory:\n",
    "1. Short Term Memory (aka Thread Scope Memory)\n",
    "2. Long Term Memory\n",
    "\n",
    "\n",
    "### **Short-term memory**\n",
    "\n",
    "Short-term memory lets your application remember previous interactions within a single thread or conversation. A thread organizes multiple interactions in a session, similar to the way email groups messages in a single conversation.\n",
    "\n",
    "LangGraph manages short-term memory as part of the agent's state, persisted via thread-scoped checkpoints. This state can normally **include the conversation history along with other stateful data, such as uploaded files, retrieved documents, or generated artifacts**. By storing these in the graph's state, the bot can access the full context for a given conversation while maintaining separation between different threads.\n",
    "\n",
    "When creating any LangGraph graph, you can set it up to persist its state by adding a checkpointer when compiling the graph:\n",
    "\n",
    "```python\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "memory = MemorySaver()\n",
    "graph.compile(checkpointer=memory)\n",
    "```\n",
    "\n",
    "#### **Steps**\n",
    "1. Define a graph - We will be using a single-node graph that calls a chat model.\n",
    "2. Add persistence - To add in persistence, we need to pass in a Checkpointer when compiling the graph.\n",
    "\n",
    "\n",
    "### **Long-term memory**\n",
    "\n",
    "Since conversation history is the most common form of representing short-term memory, in the next section, we will cover techniques for managing conversation history when the list of messages becomes long. If you want to stick to the high-level concepts, continue on to the long-term memory section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e6febcd0-9f56-4ea0-b0fe-50ee5d0d16c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Setup API Key\n",
    "f = open('keys/.openai_api_key.txt')\n",
    "OPENAI_API_KEY = f.read()\n",
    "\n",
    "# Set the OpenAI Key and initialize a ChatModel\n",
    "chat_model = ChatOpenAI(openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9428a717-76ed-40fb-a14b-90898545b04a",
   "metadata": {},
   "source": [
    "**Adding MessageState to Chat Model**  \n",
    "\n",
    "Chat models accept a list of messages as input and output a message. \n",
    "\n",
    "**LangGraph** includes a built-in MessagesState that we can use for this purpose.\n",
    "\n",
    "Below we:\n",
    "1. Define the graph state to be a list of messages;\n",
    "2. Add a single node to the graph that calls a chat model;\n",
    "3. Compile the graph with an in-memory checkpointer to store messages between runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d5acd2d5-b2d5-4a55-aaa7-57b5b57943a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<langgraph.graph.state.StateGraph object at 0x120e9f1c0>\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "# Define the function that calls the model\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: MessagesState):\n",
    "    system_prompt = (\n",
    "        \"You are a helpful assistant. \"\n",
    "        \"Answer all questions to the best of your ability.\"\n",
    "    )\n",
    "    messages = [SystemMessage(content=system_prompt)] + state[\"messages\"]\n",
    "    response = chat_model.invoke(messages)\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "# Define the two nodes we will cycle between\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "print(workflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "be5066dd-bb85-485a-b301-59a321fb7879",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# Adding memory is straight forward in langgraph!\n",
    "memory = MemorySaver()\n",
    "\n",
    "graph = workflow.compile(\n",
    "    checkpointer=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2fb4e77e-729b-4142-9c43-33e7d4d1959e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c2608a93-2f28-47c6-a83d-f15b3b0b8917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'configurable': {'thread_id': UUID('9c750511-b744-4242-9936-2d8b034c18e6')}}\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "# The thread id is a unique key that identifies\n",
    "# this particular conversation.\n",
    "# We'll just generate a random uuid here.\n",
    "# This enables a single application to manage conversations among multiple users.\n",
    "thread_id = uuid.uuid4()\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8efc4a-263c-4a45-8191-a80db274a892",
   "metadata": {},
   "source": [
    "### **.invoke()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9d14e893-7673-4978-9f8d-81a0baea71a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "input_messages = HumanMessage(content=\"Hi! I'm Kanav\")\n",
    "\n",
    "response = graph.invoke({\"messages\": input_messages}, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0a560e29-46ab-4898-8e80-2db87726f97f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content=\"Hi! I'm Kanav\", additional_kwargs={}, response_metadata={}, id='7b74f923-53b8-4c23-bfa2-3f0de6b919fc'), AIMessage(content='Hello Kanav! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 33, 'total_tokens': 45, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-3db7bbf4-7f36-4615-8c48-5d0d424aa17d-0', usage_metadata={'input_tokens': 33, 'output_tokens': 12, 'total_tokens': 45, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d5814a6d-7361-4be7-8413-69d75f398f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi! I'm Kanav\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello Kanav! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "# output contains all messages in state\n",
    "\n",
    "for msg in response[\"messages\"]:\n",
    "    msg.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34b7290-da19-4eaa-85de-957c79d918e4",
   "metadata": {},
   "source": [
    "### **.stream()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c740b695-a4b1-4431-a68a-b7acf408824e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What can you do for me?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I can help answer your questions, provide information on a wide range of topics, assist with tasks such as setting reminders or making lists, offer suggestions, and engage in general conversation. Feel free to ask me anything you'd like help with!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "input_message = HumanMessage(content=\"What can you do for me?\")\n",
    "\n",
    "for event in graph.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "56848071-b001-4fe6-b43a-522470943cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "what is my name?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your name is Kanav.\n"
     ]
    }
   ],
   "source": [
    "# Here, let's confirm that the AI remembers our name!\n",
    "input_message = HumanMessage(content=\"what is my name?\")\n",
    "\n",
    "for event in graph.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
