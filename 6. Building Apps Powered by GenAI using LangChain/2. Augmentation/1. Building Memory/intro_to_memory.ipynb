{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f5c86a2-76b6-4cf7-8f94-051684e7784d",
   "metadata": {},
   "source": [
    "# **Memory**\n",
    "\n",
    "Most LLM applications have a conversational interface. An essential component of a conversation is being able to refer to information introduced earlier in the conversation. At bare minimum, a conversational system should be able to access some window of past messages directly. A more complex system will need to have a world model that it is constantly updating, which allows it to do things like maintain information about entities and their relationships.\n",
    "\n",
    "We call this ability to store information about past interactions \"memory\". \n",
    "\n",
    "## **Building memory into a system**\n",
    "The two core design decisions in any memory system are:\n",
    "- How state is stored\n",
    "- How state is queried\n",
    "\n",
    "## **Depricated**\n",
    "- ConversationBufferMemory\n",
    "- ConversationStringBufferMemory\n",
    "- ConversationBufferWindowMemory\n",
    "- ConversationTokenBufferMemory\n",
    "- ConversationSummaryMemory\n",
    "- ConversationSummaryBufferMemory\n",
    "- VectorStoreRetrieverMemory\n",
    "\n",
    "## **What's covered?**\n",
    "- Designing Memory - Buidling End-to-end Conversational AI Bot\n",
    "- Saving and Loading a Chat History\n",
    "- SQLChatMessageHistory and Adding Session ID\n",
    "- Introduction to LangGraph for Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1210d13f-efab-42cd-bfa9-3d04865048bb",
   "metadata": {},
   "source": [
    "## **Designing Memory - Buidling End-to-end Conversational AI Bot**\n",
    "\n",
    "<img src=\"images/memory.png\">\n",
    "\n",
    "### **Steps:**\n",
    "1. Import Chat Model and Configure the API Key\n",
    "2. Create Chat Template\n",
    "3. Create a Output Parser\n",
    "4. Initialize the Memory\n",
    "5. Build a Chain\n",
    "6. Invoke the chain with human_input and chat_history\n",
    "7. Saving to memory\n",
    "8. Run Step 6 and 7 in a loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c109071-82b9-4628-a1eb-a0fcfe7588bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kanavbansal/Developer/.env_jupyter/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Step 1 - Import Chat Model and Configure the API Key\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Setup API Key\n",
    "f = open('keys/.openai_api_key.txt')\n",
    "OPENAI_API_KEY = f.read()\n",
    "\n",
    "# Set the OpenAI Key and initialize a ChatModel\n",
    "chat_model = ChatOpenAI(openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d991c8dd-ddb9-47b0-b483-ede46666e524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 - Create Chat Template\n",
    "\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        # The persistent system prompt\n",
    "        SystemMessage(\n",
    "            content=\"You are a chatbot having a conversation with a human.\"\n",
    "        ),\n",
    "        # Creating a chat_history placeholder\n",
    "        MessagesPlaceholder(\n",
    "            variable_name=\"chat_history\"\n",
    "        ),  \n",
    "        # Human Prompt\n",
    "        HumanMessagePromptTemplate.from_template(\n",
    "            \"{human_input}\"\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c5795c6-04cc-42dd-a874-29a2ab0fc2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 - Create a Output Parser\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70898aed-44e4-49cf-bd30-bf6832f4abbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4 - Initialize the Memory\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "memory_buffer = {\"history\": []}\n",
    "\n",
    "def get_history_from_buffer(human_input):\n",
    "    return memory_buffer[\"history\"]\n",
    "\n",
    "runnable_get_history_from_buffer = RunnableLambda(get_history_from_buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150d6c9f-caff-42db-8cda-1d51864ad344",
   "metadata": {},
   "source": [
    "#### **RunnablePassthrough:** RunnablePassthrough on its own allows you to pass inputs unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d6db02f-49ab-4ae3-af8d-810604ed99bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5 - Build a Chain (Another way)\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Define a chain\n",
    "chain = RunnablePassthrough.assign(\n",
    "        chat_history=runnable_get_history_from_buffer\n",
    "        ) | chat_template | chat_model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2dce5bc2-84d8-4c46-b8ac-48a2e82040f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to assist you. How can I help you today?\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 6 - Invoke the chain with human_input and chat_history\n",
    "\n",
    "query = {\"human_input\": \"Hi, How are you?\"}\n",
    "\n",
    "response = chain.invoke(query)\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "189a3dbc-59a7-4c4b-8038-2ddbc4ac5353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': []}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31f4b47a-9190-4f3d-82b6-d1b0f585a7b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi, How are you?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to assist you. How can I help you today?\", additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 7 - Saving to memory\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "memory_buffer[\"history\"].append(HumanMessage(content=query[\"human_input\"]))\n",
    "memory_buffer[\"history\"].append(AIMessage(content=response))\n",
    "\n",
    "memory_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de61677b-9f1f-4e23-a100-eb279342f5d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your input:  My name is Kanav\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*User: My name is Kanav\n",
      "*AI: Hello Kanav! It's nice to meet you. How can I assist you today?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your input:  just exploring\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*User: just exploring\n",
      "*AI: That's great! Feel free to ask me anything or share any topics you'd like to explore. I'm here to help and provide information.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your input:  that's good to know\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*User: that's good to know\n",
      "*AI: I'm glad to hear that! If you have any questions or need assistance, don't hesitate to ask. I'm here to help.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your input:  what;s my name?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*User: what;s my name?\n",
      "*AI: Your name is Kanav.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your input:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*User: exit\n"
     ]
    }
   ],
   "source": [
    "# Step 8 - Run Step 6 and 7 in a loop\n",
    "\n",
    "while True:\n",
    "    query = {\"human_input\" : input('Enter your input: ')}\n",
    "    print(f\"*User: {query['human_input']}\")\n",
    "    if query[\"human_input\"] in ['bye', 'quit', 'exit']:\n",
    "        break\n",
    "    response = chain.invoke(query)\n",
    "    print(f\"*AI: {response}\")\n",
    "\n",
    "    memory_buffer[\"history\"].append(HumanMessage(content=query[\"human_input\"]))\n",
    "    memory_buffer[\"history\"].append(AIMessage(content=response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc450acb-d899-4180-9a39-e480de710128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Hi, How are you?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to assist you. How can I help you today?\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='My name is Kanav', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"Hello Kanav! It's nice to meet you. How can I assist you today?\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='just exploring', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"That's great! Feel free to ask me anything or share any topics you'd like to explore. I'm here to help and provide information.\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content=\"that's good to know\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"I'm glad to hear that! If you have any questions or need assistance, don't hesitate to ask. I'm here to help.\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='what;s my name?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Your name is Kanav.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory_buffer[\"history\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46fd022-9da2-4d0b-9581-cf8363f36097",
   "metadata": {},
   "source": [
    "## **Saving a Chat History**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866f57c5-b8f5-4efc-a409-bf4e221629d4",
   "metadata": {},
   "source": [
    "**Let's now learn to save this history on the disk so that whenever we can load the history whenever we chat with our assistant.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8217df9c-fcd3-4f4d-baa2-85e413e7e9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "chat_history = pickle.dumps(memory_buffer)\n",
    "\n",
    "with open(\"chats_data/conversation_memory.pkl\", \"wb\") as f:\n",
    "    f.write(chat_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c5c8b1-4521-43ce-83f2-f1c84d3609d0",
   "metadata": {},
   "source": [
    "## **Loading a Chat History**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9ca65b7-c292-45f2-92e4-20f84679277a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history_loaded = pickle.load(open(\"chats_data/conversation_memory.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35fb23db-84d4-45cf-82a9-1e46184fba98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi, How are you?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to assist you. How can I help you today?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='My name is Kanav', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Hello Kanav! It's nice to meet you. How can I assist you today?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='just exploring', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's great! Feel free to ask me anything or share any topics you'd like to explore. I'm here to help and provide information.\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"that's good to know\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"I'm glad to hear that! If you have any questions or need assistance, don't hesitate to ask. I'm here to help.\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='what;s my name?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Your name is Kanav.', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history_loaded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a57196c-f3b8-49d3-b812-8c4d999e6adf",
   "metadata": {},
   "source": [
    "## **SQLChatMessageHistory**\n",
    "\n",
    "`ChatMessageHistory` allows us to store separate conversation histories per user or session which is often done by the real-time chatbots. `session_id` is used to distinguish between separate conversations.\n",
    "\n",
    "In order to use it, we can use a `get_session_history` function which take `session_id` and returns a message history object.\n",
    "\n",
    "There is a support of many `Memory` components under `langchain_community.chat_message_histories`, like:\n",
    "1. AstraDBChatMessageHistory\n",
    "2. DynamoDBChatMessageHistory\n",
    "3. CassandraChatMessageHistory\n",
    "4. ElasticsearchChatMessageHistory\n",
    "5. KafkaChatMessageHistory\n",
    "6. MongoDBChatMessageHistory\n",
    "7. RedisChatMessageHistory\n",
    "8. PostgresChatMessageHistory\n",
    "9. SQLChatMessageHistory\n",
    "\n",
    "**[Click Here](https://python.langchain.com/v0.2/docs/integrations/memory/)** to read more.\n",
    "\n",
    "### **Usage**\n",
    "\n",
    "To use the storage you need to provide only 2 things:\n",
    "\n",
    "1. **Session Id** - a unique identifier of the session, like user name, email, chat id etc.\n",
    "2. **Connection string**\n",
    "    - For SQL (SQLAlchemy) - A string that specifies the database connection. It will be passed to SQLAlchemy create_engine function.\n",
    "    - For SQLite - A string that specifies the database connection. For SQLite, that string is slqlite:/// followed by the name of the database file. If that file doesn't exist, it will be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "def066d9-117f-481a-942a-69d0dd8d483e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import SQLChatMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8de1ed9-082b-40ce-bb96-69dc61022146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the OpenAI Key and initialize a ChatModel\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat_model = ChatOpenAI(openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73fcc4a9-2e7a-4f83-bc25-4c95d573ec74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a connection with the database and \n",
    "# return the chat message history for a session id\n",
    "\n",
    "from langchain_community.chat_message_histories import SQLChatMessageHistory\n",
    "\n",
    "def get_session_message_history_from_db(session_id):\n",
    "    chat_message_history = SQLChatMessageHistory(\n",
    "                                   session_id=session_id, \n",
    "                                   connection=\"sqlite:///chats_data/sqlite.db\"\n",
    "                               )\n",
    "    return chat_message_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b5043c4-3fa1-484f-beae-937766b8741e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a chat template\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "chat_template = ChatPromptTemplate(\n",
    "                messages=[\n",
    "                    (\"system\", \"You are a helpful AI assistant.\"), \n",
    "                    MessagesPlaceholder(variable_name=\"history\"), \n",
    "                    (\"human\", \"{human_input}\")\n",
    "                ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af3bac9b-9370-4a60-8fad-23b175e1065c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the chain\n",
    "\n",
    "chain = chat_template | chat_model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "989e9909-67fc-4afa-a369-ef0e231b856e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use RunnableWithMessageHistory to load \n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "conversation_chain = RunnableWithMessageHistory(\n",
    "                        chain, \n",
    "                        get_session_message_history_from_db,\n",
    "                        input_messages_key=\"human_input\", \n",
    "                        history_messages_key=\"history\"\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7e00998b-2fe4-4ada-a48c-7da04c4da47b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes, ThatAIGuy, the capital of Himachal Pradesh is Shimla.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is where we configure the session id\n",
    "user_id = \"thataiguy\"\n",
    "config = {\"configurable\": {\"session_id\": user_id}}\n",
    "\n",
    "input_prompt = {\"human_input\": \"My name is ThatAIGuy. Can you tell me the capital of Himachal?\"}\n",
    "response = conversation_chain.invoke(input_prompt, config=config)\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "361e0eaa-bd4d-4617-94be-ba5a2e0311a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The biggest state in India by area is Rajasthan.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is where we configure the session id\n",
    "user_id = \"kanav\"\n",
    "config = {\"configurable\": {\"session_id\": user_id}}\n",
    "\n",
    "input_prompt = {\"human_input\": \"My name is Kanav Bansal. What is the biggest state in India?\"}\n",
    "response = conversation_chain.invoke(input_prompt, config=config)\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "815f32f8-c0ec-4cd8-a52a-80e9adda6daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_bot(session_id, prompt):\n",
    "    config = {\"configurable\": {\"session_id\": user_id}}\n",
    "    input_prompt = {\"human_input\": prompt}\n",
    "\n",
    "    response = conversation_chain.invoke(input_prompt, config=config)\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "78c27159-25a8-43bd-ad05-6a8bba6f91a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes, your name is ThatAIGuy.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_id = \"thataiguy\"\n",
    "input_prompt = \"Do you remember my name?\"\n",
    "chat_bot(session_id=user_id, prompt=input_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a8122a47-96a9-4afd-a328-7e28909594b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes, your name is Kanav Bansal.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_id = \"kanav\"\n",
    "input_prompt = \"Do you remember my name?\"\n",
    "chat_bot(session_id=user_id, prompt=input_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e619479-d835-4290-ac7f-f0077badd475",
   "metadata": {},
   "source": [
    "## **Introduction to LangGraph for Memory**\n",
    "\n",
    "As of the v0.3 release, LangChain recommends that users take advantage of LangGraph persistence to incorporate memory into new LangChain applications.\n",
    "\n",
    "If your code is already relying on `RunnableWithMessageHistory` or `BaseChatMessageHistory`, you do not need to make any changes. We do not plan on deprecating this functionality in the near future as it works for simple chat applications and any code that uses `RunnableWithMessageHistory` will continue to work as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e6febcd0-9f56-4ea0-b0fe-50ee5d0d16c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Setup API Key\n",
    "f = open('keys/.openai_api_key.txt')\n",
    "OPENAI_API_KEY = f.read()\n",
    "\n",
    "# Set the OpenAI Key and initialize a ChatModel\n",
    "chat_model = ChatOpenAI(openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9428a717-76ed-40fb-a14b-90898545b04a",
   "metadata": {},
   "source": [
    "**Adding MessageState to Chat Model**  \n",
    "\n",
    "Chat models accept a list of messages as input and output a message. \n",
    "\n",
    "**LangGraph** includes a built-in MessagesState that we can use for this purpose.\n",
    "\n",
    "Below we:\n",
    "1. Define the graph state to be a list of messages;\n",
    "2. Add a single node to the graph that calls a chat model;\n",
    "3. Compile the graph with an in-memory checkpointer to store messages between runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d5acd2d5-b2d5-4a55-aaa7-57b5b57943a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<langgraph.graph.state.StateGraph object at 0x11a856cd0>\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: MessagesState):\n",
    "    response = chat_model.invoke(state[\"messages\"])\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": response}\n",
    "\n",
    "# Define the two nodes we will cycle between\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "print(workflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "be5066dd-bb85-485a-b301-59a321fb7879",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# Adding memory is straight forward in langgraph!\n",
    "memory = MemorySaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f6c2bde9-86a5-473b-b126-2dacfb22c8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = workflow.compile(\n",
    "    checkpointer=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2fb4e77e-729b-4142-9c43-33e7d4d1959e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGsAAACGCAIAAABVB+MHAAAAAXNSR0IArs4c6QAAD4ZJREFUeJztnXlwE9cdgJ+0OleXdRjhE9uAMWBjCCY1YIIBQ4lj7HgINQWnJA20tEx6QNo0MxCSMkMSN9PSgSlJC6ElOCEkkLoKGXACmMMOhyEF25jLsrEtCXRrtbp3pf4halKse1do7er7z9r33v786e3qXbuP5vP5QBIC0BMdwIgnaZAoSYNESRokStIgUZIGicIgmN9q9FgMHrsVtyM45vF5vSOgbQQxAINBh4UQLGCIxzJhPiEJtNjagwaNq+earbfDxoJpwEeDBRAshLg8hhcfAQYZTBqKYHYEt1sxl8PLZNHzingTivlCKTOG0qI2iJqxNoXeB0CKjJlbxBuTyYnhrJRC0+tQdthM9918MWNOlYzFie7OFp3BS83GzjbLnGWySTMF0YdKdTrOWdq+0Jc+Iy2elxJ5rigMNu1WTZjBn1oqijXCkcHlr42Ge+4l9WMjTB9pjd27pXfGQvGo1wcAmFkhGVfAa9qtijSDLwL2bFbq1c5IUo4abv/bevDd/khShr+Km3arZiwUZ0+CSfh+RxTdFxCV0lHxQ3noZGEMtn9l5PKhqbNH/8UbkPavjVxemH8/1H0QNWMdrZb/W30AgJIKyalDutBpQhlsU+jnLJORHdUIY3aVtE2hD5EgqEGDxuUDYFS2+6Ji5iKxXu1y2rBgCYIa7LlmS5HF0suJjc7OTpfLlajsoeEJGcpOe7CjQQ32dthyi3hxiukRFArFCy+84HA4EpI9LHlFfGUHGuxoYIOI0cOG6Y+tzxtz9fE3JOJX+/zkFvJQExZs2CmIQYMnTlN4d+/eXb9+fVlZWWVl5fbt271er0KhePvttwEAFRUVJSUlCoUCAHD//v2tW7dWVFSUlpbW1dUdO3bMn91sNpeUlHz44YebN28uKytbt25dwOykg3l8Fr0n4KHAQ2N2Kw4LoHiEsm3btr6+vk2bNtlstvb2djqdPnfu3Pr6+gMHDuzYsYPP52dnZwMAMAzr6up67rnnUlJSTp48uXnz5qysrKlTp/oL2bt374oVK9577z0IguRy+fDspAMLITuCi8cEOBTEIILDwrgYVKvVBQUFtbW1AID6+noAgEQiyczMBAAUFhampDwYFMnIyPj0009pNBoAoKampqKioqWlZchgUVHRhg0bhsocnp10eEKGDQn8cxz0l4TJissEQGVl5fnz5xsaGoxGY+iUt27d2rhx49KlS2tra3EcNxgMQ4eefPLJeMQWAhaHHqzzFlgTh0e3moK2gIiwYcOGjRs3Njc3V1dXHzp0KFiyS5curVmzxu12b926taGhQSQSeb3eoaNcLjcesYXAovfAgsDXa+BPYQHDbo2LQRqNtmrVqpqamu3btzc0NOTn50+fPt1/6Ltf8p49ezIzM3fs2MFgMCJUFtflKyF+GALXQb4YYnPjchX7Wx48Hm/9+vUAgBs3bgwJ0uke9kDNZnN+fr5fn9vtttvt362DjzA8O+nwRJBAHLh/EbgOSuRs3aDbrHOnpLLIDeXVV1/l8/mlpaXnzp0DAEyePBkAUFxcDEHQu+++W11d7XK5li9f7m+XNDU1iUSixsZGBEF6enqC1bLh2cmNWXXH4cVAsPkT6I033gh4wGrCbBYsLZfkO87g4OC5c+eOHTvmcDhefvnl8vJyAIBQKJTL5V999dXZs2cRBKmqqiouLlYqlQcPHmxvb1+8eHFdXd3x48cLCgqkUun+/fvLysqmTJkyVObw7OTGfPW0WZ7DGZsTuH8RdHxQrXR0X0AWhRtf/H/g6F5NWY1MFGSUIOhkc3oe9+Ix48Ate1Z+4NFpBEGqq6sDHsrMzBwcHBz++fz58998882II4+RtWvX3rlzZ/jnkydP7u7uHv55YWHhrl27gpXWfRFhc+nB9IUZo9YOOE8d0tVtygp41Ov13rt3L3ChtMDFcrlcsVgc7HRkodPpPJ4APbBgUbFYLJks6DDo3i29P/xtVrCmTPhR/jOf67Lz4Zypj2mQhmp0nbfYEXzWEkmINGGaLE/Vpp4+okMMgTvVoxt1j+PGJWtofSCS2U6XE3/vt3fImEEcSThsnvd/1xNJyojmi90u/P3X7qAWD+HARgbaQefe15UY5o0kcaSrPhwo/nFD//d/JM+YMMonju9ctbY3m1b+JtJRsuhWHp36RIuYPHOXyWQZ7FgjpC6qHsc3CoN8HHtebWrkuaJe/dZ/w96q0GcXwPIsTm4hD2LQog+VWridXmUneq/PadS4Zy+TpuVE1w2LcQVmzzX01hVrb6dt0kwBk03nCRk8EcSBoZGwhBVAdJrditkQzIbgqMUzeMuRV8jPL+GPK4il0RajwSH6b9hNWrcNwWwW3Ov1YW4yFeI43tHRMTT8RRZsmO4fduYJIWkai+CdnajBuIKiaFVVVUtLS6IDCUVyLT9RkgaJQnWD/iFYKkN1gwHHoygF1Q3GbwqYLKhu0Gw2JzqEMFDdYHp6eqJDCAPVDarV6kSHEAaqGywqKkp0CGGgusGOjo5EhxAGqhukPlQ3GGIWjSJQ3aBeH+pJBCpAdYOpqVEMFycEqhuM64osUqC6QepDdYMTJkxIdAhhoLrBgGuIKAXVDVIfqhv87kpLakJ1g9evX090CGGgukHqQ3WDybEZoiTHZkY/VDeYnO0kSnK2c/RDdYPJ+WKiJOeLiTJx4sREhxAGqhu8fft2okMIA9UNUh+qGxw7NtJ3USYKqhsM9vAjdaC6wcLCwkSHEAaqG+zs7Ex0CGGgusFkHSRKsg4SJSsr8BP21IGKT+SsW7dOrVYzGAyv16vX62UyGZ1O93g8X375ZaJDCwAV6+Dq1asRBFGpVBqNxuPxaDQalUoFQXF5kxpxqGiwvLz8ke6wz+ej7IQJFQ0CAJ5//nkYfvjAYFpa2sqVKxMaUVAoanDBggW5ublD9+ji4uJp06YlOqjAUNQgAODFF1/0D6/KZDLKVkBKGywvL8/Ly/NPGVP2JkjCPk2RgHu8DpvXjmBOO45F81bDZ5f81GX6pLL8RWWnLfJcDCaNy4NgIQTzIRo97i8xiGN70Kxz93XZb32Lelw+uxVjcSG+mONyxOXFkN+FyYJsFpfbgfPFTA5Mz5/OGzcFDvb2QOLExaBJ6z5zxGAxYGw+my+DeZLH/dLPIax6O6q3e90e6VjmvFopT0j+NUe+wa8/0t29aU/NEwvHUOhtXWa1VdtjmlIqLKuWklsymQYdKH7grf7U8ZKUND5ZZZKLSYXYdNb618h8ZzVpBq0mz0fvDOSVZjDZj+PXKWYciKvnvPonb+VFu6taMMgxaNC4ju7TZs+g+pOsfnw+3912dd2mdC6PhC+bhO/B6/V9/IeBkaLP/yrHjGnyxrcGyCmNeB08vFPFT5OyeY9vMxNSsJscPgf6zEtE5wKJ1sHLJ0yYjzni9AEAYDHXbPTd/tZKsByiBs8fNYwZH+4tkVQldbz47D8NESQMBSGDl5qNaQWSx9BzihMsLlMo53V9YyFSCCGDHa0IX/a4m80X2pte2fI9BAn11KzNZn5ly/faLh4OWxo3Be5oI3Qhx25Qr3bRIDqLS+nWX1h4Yo5Z63ba8JhLiN1gbyfKl42GN4qmpMF9XVGM/TxC7DVI0+dmweEv4X2NvxmTmuPxONu/Perz+SaOn1U2u+7E6X19/dcEfOn3F/5k5vSn/SnvDnR+cXzngOo6i8WdOmnesqW/hGGh/5BKffOfX/5xQHVdKJClSv+nT9Z28fDp1o8siFYiTp8xbUn53HomM7o3nDK5rHv9roJYN42JvQ7aEYzJjmj+7NTZ/QCA9T/+S3lZfWf36b/94xeFBfN/9uPd6Wn5B4/8/r6uDwBwT6t8f98GHPfU1W5ZXP5SR3fL/k9e82e/r+vb/cHPEERXufjn8+esUmluDpXcfPJvR4/vml60+AfPbp42dVHL2QOfNb0V7T/CYDOIbGcTex10oLiEFZFBeWrus89sAgBkphdcuPyv7Mwpc0tXAABqnv515/UWZe8VeWrOiZZ9NBp93Y/+zOUKAAAwV/jx4Td6eq+Mz33i6PGdNBr95Z/u5fPEAAAanX5E0QAAsCC6E2f+vvq5bdMKF/pPJBLIDiveqancGNU/wmBBVkPs98HYDXL4DDojoirMYDy8rJhMNgQ9aH6niOQAAJvdDADo6bsyIa/Erw8AMGliKQBgQNWdlTHl5p3zs2ct9+sDAED0BzHf7rmI41jjZ683fvb6f4v3AQAsVq2QH8UbQiAmncmO/VqM3SDm9mIunBFZNQyIfyswf7fS6USHHAEAuBwhAACx6hGrHscxiThteHbEqgcAvFT/xxTR/+yCJpVkOp1BtzgcjseJAQJd29gNwgIIc8de+R9BJBxjczxs2aI2IwCAy+H7taKoaXgWLvfB78yY1Bwip8bcOF8Uu4fYa68sg417SDM4LrtI2XvF7Xb6/7zWdRIAkDtuOofDk0mzrnadwLBHd1eYmFdCo9HOXXi44ZjL/WDrTv+Nwu5AIjm1F/dJ02Pv18ducOw4FqoPuqNqtCx66gWX27Fn/6+uXD1+8sw/jjbvmpA7c3zuEwCAJQvWGoyDO/+6tvX8p20XD7e0NvqzyKRZZaV112+c/eDApguX//V1ywdv/2n5oPoGAIDD4UklmWdaP/rm0udhT41q0XQCmynFbjCvkI9oSTOYKstet+bPGO755PNtLa2NM4ufXrOqwX+jfKJ4ae0zr9gdli+ad168rBiX9XBNZvXTv1q29Bea+z1HFO9cuNxUOKVcJHxwT1y94vcyafblb8Ms98I9uMPqSR8fu0FC44Nf7LlH4/ITOBVHHLMGFcCuRSsD7cgZGYRGFqbNExoHqP7UVmiM/eYZCwg9ukfIYPYkmMOloYZ4bb0cbywaa3ouWyIntCcf0RHWp2olqDainzwKgupR4tPHRA3Ks7nji7h6ZZiNYCmI5rr2iQVCPuHVICTM1c1aLIZh3KwaSTVR12vMyGNOniUkXhRpM+4nD+nMFkiSKSKltLii7THm5jNmLSFn5zzS1g8u/EEqj+PWK4lO3MSb+ze1aZk0svSRv/LoyinTrSs2/hgRBYevES3qMKLTnxJOmikgsVjy127p1c5WhREx4qJ0kSAV9vcrEogX81oNDlO/aWwOe06VVCgheWo7Xisw1UrH1bOWnn+j4nSYK4YhiMZgM1gcBoi/T5/X53FiHhfu8/lsOtRqcE2aJSguE0nT47K/Wdyfaertsmn7nTqVB7VgEIOO6N1xPR0AQCBh+nw+fgpDnsmS53CC7X1LFlR8KmxkQd21/COFpEGiJA0SJWmQKEmDREkaJMp/AGOWqtryQOtzAAAAAElFTkSuQmCC",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(app.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c2608a93-2f28-47c6-a83d-f15b3b0b8917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'configurable': {'thread_id': UUID('49673ef4-7c87-4d27-8c3c-15ef14500448')}}\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "# The thread id is a unique key that identifies\n",
    "# this particular conversation.\n",
    "# We'll just generate a random uuid here.\n",
    "# This enables a single application to manage conversations among multiple users.\n",
    "thread_id = uuid.uuid4()\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8efc4a-263c-4a45-8191-a80db274a892",
   "metadata": {},
   "source": [
    "### **.invoke()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9d14e893-7673-4978-9f8d-81a0baea71a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_messages = HumanMessage(content=\"Hi! I'm Kanav\")\n",
    "\n",
    "response = app.invoke({\"messages\": input_messages}, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0a560e29-46ab-4898-8e80-2db87726f97f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content=\"Hi! I'm Kanav\", additional_kwargs={}, response_metadata={}, id='816e8cdd-ae0c-4279-b6e6-0f353836e3fd'), AIMessage(content='Hello Kanav! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 13, 'total_tokens': 25, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-da2c77c3-1478-44cb-ac4c-bec60ad6cda6-0', usage_metadata={'input_tokens': 13, 'output_tokens': 12, 'total_tokens': 25, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d5814a6d-7361-4be7-8413-69d75f398f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi! I'm Kanav\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello Kanav! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "# output contains all messages in state\n",
    "\n",
    "for msg in response[\"messages\"]:\n",
    "    msg.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34b7290-da19-4eaa-85de-957c79d918e4",
   "metadata": {},
   "source": [
    "### **.stream()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c740b695-a4b1-4431-a68a-b7acf408824e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi! I'm Kanav\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello Kanav! Nice to meet you. How are you doing today?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "input_message = HumanMessage(content=\"Hi! I'm Kanav\")\n",
    "\n",
    "for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "56848071-b001-4fe6-b43a-522470943cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "what was my name?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your name is Kanav.\n"
     ]
    }
   ],
   "source": [
    "# Here, let's confirm that the AI remembers our name!\n",
    "input_message = HumanMessage(content=\"what was my name?\")\n",
    "\n",
    "for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
