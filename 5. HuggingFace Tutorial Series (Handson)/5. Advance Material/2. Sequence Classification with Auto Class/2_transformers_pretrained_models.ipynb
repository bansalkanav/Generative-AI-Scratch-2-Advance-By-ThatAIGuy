{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1130b0fb-9093-4cec-8d44-b1c606cfa434",
   "metadata": {},
   "source": [
    "# **Inference using Pipeline vs Auto Class**\n",
    "\n",
    "**Agenda**  \n",
    "1. Inference using Pipeline\n",
    "2. Digging Deeper using Auto Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89dd10d-87f7-4fbd-806a-31f9743a4d4d",
   "metadata": {},
   "source": [
    "### **Inference using Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d910a99b-6ce2-4b05-ac1a-d0ff73766e7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e641747d9314f58ab3dae8904543e5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/929 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0c82c8d49b84bd38e243f3dcbc6b894",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/501M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aa6f81a957a474092473ef8ad4e4786",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3103af9aec834cd59a864b91c3e53879",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c5b5526e86b4c108b3cd42269f3c35a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(task=\"text-classification\", model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91bd4ee0-eecc-4bb2-80db-2a494ea14523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'negative', 'score': 0.9429681897163391}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet = \"A very bad experience at the airport.\"\n",
    "\n",
    "classifier(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c92456-5a4c-4798-a0e1-dd5c311baf48",
   "metadata": {},
   "source": [
    "### **Dig Deeper using Auto Class**\n",
    "This gives more transperancy and visibility in the whole pipeline.\n",
    "\n",
    "**Reference - [Auto Class Documentation](https://huggingface.co/docs/transformers/autoclass_tutorial)**\n",
    "\n",
    "1. **AutoTokenizer -** Nearly every NLP task begins with a tokenizer. A tokenizer converts your input into a format that can be processed by the model. Load a tokenizer with `AutoTokenizer.from_pretrained()`.\n",
    "2. **AutoModel -** The AutoModelFor classes let you load a pretrained model for a given task. For example, load a model for sequence classification with `AutoModelForSequenceClassification.from_pretrained()`.  **[Click here](https://huggingface.co/docs/transformers/model_doc/auto)** for a complete list of available tasks under AutoModel Class.\n",
    "3. **AutoImageProcessor -** For vision tasks, an image processor processes the image into the correct input format. Use `AutoImageProcessor.from_pretrained()`.\n",
    "4. **AutoFeatureExtractor -** For audio tasks, a feature extractor processes the audio signal the correct input format. Load a feature extractor with `AutoFeatureExtractor.from_pretrained()`.\n",
    "5. **AutoProcessor -** Multimodal tasks require a processor that combines two types of preprocessing tools. For example, the LayoutLMV2 model requires an image processor to handle images and a tokenizer to handle text; a processor combines both of them. Load a processor with `AutoProcessor.from_pretrained()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "659a483b-9f98-40e8-bb50-7e233baaa96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment-latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cde9f47d-def7-4474-90c9-1a06fa1c370e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaTokenizerFast(name_or_path='cardiffnlp/twitter-roberta-base-sentiment-latest', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t50264: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "abc6b57d-4fd4-4fc6-a09b-be653f0a7a5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58252236-fa87-4954-9780-05c3ce5878b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:\n",
      " ['A', 'Ġvery', 'Ġbad', 'Ġexperience', 'Ġat', 'Ġthe', 'Ġairport', '.']\n"
     ]
    }
   ],
   "source": [
    "tweet = \"A very bad experience at the airport.\"\n",
    "\n",
    "tokens = tokenizer.tokenize(tweet)\n",
    "\n",
    "print(\"Tokens:\\n\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1a331045-e47b-49dd-af37-868e0fa346c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Ids:\n",
      " {'input_ids': [0, 250, 182, 1099, 676, 23, 5, 3062, 4, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "tweet = \"A very bad experience at the airport.\"\n",
    "\n",
    "token_ids = tokenizer(tweet)\n",
    "\n",
    "print(\"Token Ids:\\n\", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d7a731c8-01f3-4d33-858e-60a140f33d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded Text Output:\n",
      " <s>A very bad experience at the airport.</s>\n"
     ]
    }
   ],
   "source": [
    "## Decode method allows us to check how the final output of the \n",
    "## tokenizer translates back to text\n",
    "print(\"Decoded Text Output:\\n\", tokenizer.decode(token_ids[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "14393908-720f-46b0-93d8-096c88ecd3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0,   250,   182,  1099,   676,    23,     5,  3062,     4,     2,\n",
      "             1,     1],\n",
      "        [    0, 41710,   633,   626,    30,     5,   168,    19,    49,  5287,\n",
      "             4,     2],\n",
      "        [    0,   713,    16,    95,    10,  9624,  6755,     4,     2,     1,\n",
      "             1,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "# A tokenizer can also accept a list of inputs, and pad and truncate the text to return a batch with uniform length:\n",
    "tweets = [\"A very bad experience at the airport.\", \n",
    "          \"Amazing job done by the government with their initiatives.\", \n",
    "          \"This is just a random string.\"]\n",
    "\n",
    "token_ids = tokenizer(tweets, padding=True, truncation=True, max_length=15, return_tensors=\"pt\")\n",
    "\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fe9f861f-76e9-4a49-91ea-94337e2d9ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.5498, -0.3649, -2.5233],\n",
      "        [-2.2652, -1.1891,  3.2718],\n",
      "        [ 0.5962,  1.1877, -1.9644]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "# Now pass your preprocessed batch of inputs directly to the model. \n",
    "# You just have to unpack the dictionary by adding **\n",
    "outputs = model(**token_ids)\n",
    "\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "515bc583-3f75-4170-96f7-eab5abddb14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9430, 0.0511, 0.0059],\n",
      "        [0.0039, 0.0114, 0.9847],\n",
      "        [0.3468, 0.6265, 0.0268]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "predictions = nn.functional.softmax(outputs.logits, dim=-1)\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "34e8ac51-bc76-4437-bd0c-d2112c36d326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: A very bad experience at the airport.\n",
      "Prediction: negative\n",
      "\n",
      "Tweet: Amazing job done by the government with their initiatives.\n",
      "Prediction: positive\n",
      "\n",
      "Tweet: This is just a random string.\n",
      "Prediction: neutral\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the index of the maximum value (the predicted class)\n",
    "predicted_class_idx = predictions.argmax(dim=-1)\n",
    "\n",
    "# Define the mapping from indices to class labels\n",
    "class_labels = ['negative', 'neutral', 'positive']\n",
    "\n",
    "# Convert the predicted indices to the corresponding labels\n",
    "predicted_labels = [class_labels[idx] for idx in predicted_class_idx]\n",
    "\n",
    "# Print the predicted labels\n",
    "for i in range(len(predicted_labels)):\n",
    "    print(f\"Tweet: {tweets[i]}\")\n",
    "    print(f\"Prediction: {predicted_labels[i]}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
